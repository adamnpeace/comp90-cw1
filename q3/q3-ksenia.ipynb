{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -fsS http://udon.stacken.kth.se/\\~ninjin/comp0090_assignment_1_data.tar.gz -o /tmp/data.tar.gz\n",
    "!tar -x -z -f /tmp/data.tar.gz\n",
    "!rm -f /tmp/data.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainxs = np.load(\"comp0090_assignment_1_data/fashion-train-imgs.npz\").transpose((2, 1, 0))\n",
    "trainys = np.load(\"comp0090_assignment_1_data/fashion-train-labels.npz\")\n",
    "devxs   = np.load(\"comp0090_assignment_1_data/fashion-dev-imgs.npz\").transpose((2, 1, 0))\n",
    "devys   = np.load(\"comp0090_assignment_1_data/fashion-dev-labels.npz\")\n",
    "testxs  = np.load(\"comp0090_assignment_1_data/fashion-test-imgs.npz\").transpose((2, 1, 0))\n",
    "testys  = np.load(\"comp0090_assignment_1_data/fashion-test-labels.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12000, 28, 28)\n",
      "(12000,)\n",
      "(1000, 28, 28)\n",
      "(1000,)\n",
      "(1000, 28, 28)\n",
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "print(trainxs.shape)\n",
    "print(trainys.shape)\n",
    "print(devxs.shape)\n",
    "print(devys.shape)\n",
    "print(testxs.shape)\n",
    "print(testys.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "f, axarr = plt.subplots(4,4)\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        axarr[i, j].imshow(trainxs[np.random.randint(0, len(trainxs))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Analytical Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derived them on ipad, i'd rather confirm the results with someone before typing it all out cause it's painful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Verifying Gradients Correctness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Queue Adam's code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Implementing the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid for a single sample, preventing overflow\n",
    "def sigmoid(z):\n",
    "#     return 1. / (1. + np.exp(-z))\n",
    "    return np.exp(np.fmin(z, 0)) / (1 + np.exp(-np.abs(z)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid derivative\n",
    "def sigmoid_d(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward pass\n",
    "def forward(params, x):\n",
    "    x = x.reshape(28**2, 1)\n",
    "    \n",
    "    forward_res = {}\n",
    "    forward_res[\"z1\"] = params[\"W1\"] @ x + params[\"b1\"]\n",
    "    forward_res[\"a1\"] = sigmoid(forward_res[\"z1\"])\n",
    "    forward_res[\"z2\"] = params[\"W2\"] @ forward_res[\"a1\"] + params[\"b2\"]\n",
    "    forward_res[\"a2\"] = sigmoid(forward_res[\"z2\"])\n",
    "    forward_res[\"z3\"] = params[\"W3\"] @ forward_res[\"a2\"] + params[\"b3\"]\n",
    "    forward_res[\"a3\"] = sigmoid(forward_res[\"z3\"])\n",
    "    \n",
    "    return forward_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idk what those are and at this point i'm too scared to ask\n",
    "def loss(y_pred, y):\n",
    "    if y_pred == 0 or y_pred == 1:\n",
    "        return np.array([[0]])\n",
    "    return (-y * np.log(y_pred)) - ((1 - y) * np.log(1 - y_pred))\n",
    "\n",
    "def loss_prime(y_pred, y):\n",
    "    return (-y/y_pred) + (1-y)/(1-y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backpropagation\n",
    "def backprop(x, y, params, forward_res):\n",
    "    x = x.reshape(28**2, 1)\n",
    "    \n",
    "    grads = {}\n",
    "    \n",
    "    z1 = params[\"W1\"].dot(x) + params[\"b1\"]\n",
    "    a1 = sigmoid(z1)\n",
    "    \n",
    "    z2 = params[\"W2\"].dot(a1) + params[\"b2\"]\n",
    "    a2 = sigmoid(z2)\n",
    "    \n",
    "    z3 = params[\"W3\"].dot(a2) + params[\"b3\"]\n",
    "    y_pred = a3 = sigmoid(z3)\n",
    "    \n",
    "    dL_dz3 = loss_prime(y_pred, y) * sigmoid_d(z3) # (1, 1)\n",
    "    dL_dz2 = dL_dz3.dot(params[\"W3\"]).T * sigmoid_d(z2) # (20, 1)\n",
    "    dL_dz1 = np.multiply(dL_dz2.T.dot(params[\"W2\"]).T, sigmoid_d(z1)) # (200, 1)\n",
    "    \n",
    "    grads[\"dW3\"] = dL_dz3.dot(a2.T) # (1, 20) <- not sure those are right didn't check\n",
    "    grads[\"db3\"] = dL_dz3 # (1, 1)\n",
    "    \n",
    "    grads[\"dW2\"] = dL_dz2.dot(a1.T) # (20, 200)\n",
    "    grads[\"db2\"] = dL_dz2 # (20, 1)\n",
    "    \n",
    "    grads[\"dW1\"] = dL_dz1.dot(x.T) # (784, 200)\n",
    "    grads[\"db1\"] = dL_dz1 # (200, 1)\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_params(grads, params, lr, momentum, dW1, db1, dW2, db2, dW3, db3):\n",
    "#     I DO NOT KNOW WHAT THIS IS\n",
    "# THIS IS WITH MOMENTUM BUT I DONT THINK WE NEED IT UWU\n",
    "    # with momentum (optional)\n",
    "#     if epoch == 0:\n",
    "#                dW1 = 0\n",
    "#                db1 = 0\n",
    "#                ...\n",
    "    # use momentum 0 so dW1 = grads[\"dW1\"], db1 = grads[\"db1\"]...\n",
    "    dW1 = momentum * dW1 + (1 - momentum) * grads[\"dW1\"]\n",
    "    db1 = momentum * db1 + (1 - momentum) * grads[\"db1\"]\n",
    "    dW2 = momentum * dW2 + (1 - momentum) * grads[\"dW2\"]\n",
    "    db2 = momentum * db2 + (1 - momentum) * grads[\"db2\"]\n",
    "    dW3 = momentum * dW3 + (1 - momentum) * grads[\"dW3\"]\n",
    "    db3 = momentum * db3 + (1 - momentum) * grads[\"db3\"]\n",
    "\n",
    "    params[\"W1\"] += (lr * dW1)\n",
    "    params[\"b1\"] += lr * db1\n",
    "    params[\"W2\"] += (lr * dW2)\n",
    "    params[\"b2\"] += lr * db2\n",
    "    params[\"W3\"] += (lr * dW3)\n",
    "    params[\"b3\"] += lr * db3\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_grads(grads, ups, batch_size):\n",
    "    grads[\"dW3\"] -= ups[\"dW3\"] / batch_size\n",
    "    grads[\"db3\"] -= ups[\"db3\"] / batch_size\n",
    "    \n",
    "    grads[\"dW2\"] -= ups[\"dW2\"] / batch_size\n",
    "    grads[\"db2\"] -= ups[\"db3\"] / batch_size\n",
    "    \n",
    "    grads[\"dW1\"] -= ups[\"dW1\"] / batch_size\n",
    "    grads[\"db1\"] -= ups[\"db1\"] / batch_size\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "def mlp(xs, ys, learning_rate, momentum, devxs, devys):\n",
    "    img_size = 28**2\n",
    "    h1 = 200\n",
    "    h2 = 20\n",
    "    out_size = 1\n",
    "    \n",
    "    #     stores training set loss for part 3.5\n",
    "    loss_training = []\n",
    "    \n",
    "    #     stores training set accuracy for part 3.6\n",
    "    accuracy_training = []\n",
    "    \n",
    "    #     stores validation set accuracy for part 3.7\n",
    "    accuracy_validation = []\n",
    "    \n",
    "    # initialise parameters\n",
    "    params = {\"W1\": np.random.randn(h1, img_size) * np.sqrt(1 / img_size),\n",
    "              \"b1\": np.zeros((h1, 1)),\n",
    "              \"W2\": np.random.randn(h2, h1) * np.sqrt(1 / h1),\n",
    "              \"b2\": np.zeros((h2, 1)),\n",
    "              \"W3\": np.random.randn(out_size, h2) * np.sqrt(1 / h2),\n",
    "              \"b3\": np.zeros((out_size, 1))}\n",
    "    \n",
    "    grads = {}\n",
    "    \n",
    "    loss_old = validation_loss_c(devxs, devys, params)\n",
    "    print('loss_old', loss_old)\n",
    "    \n",
    "    epoch = 0\n",
    "    row_epoch = 0\n",
    "        \n",
    "    while True:\n",
    "        # shuffle training set\n",
    "        permutation = np.random.permutation(xs.shape[0])\n",
    "        trainxs_shuffled = xs[permutation]\n",
    "        trainys_shuffled = ys[permutation]\n",
    "        \n",
    "        grads[\"dW3\"]  = np.zeros_like(params[\"W3\"])\n",
    "        grads[\"db3\"] = np.zeros_like(params[\"b3\"])\n",
    "\n",
    "        grads[\"dW2\"] = np.zeros_like(params[\"W2\"])\n",
    "        grads[\"db2\"] = np.zeros_like(params[\"b2\"])\n",
    "\n",
    "        grads[\"dW1\"] = np.zeros_like(params[\"W1\"])\n",
    "        grads[\"db1\"] = np.zeros_like(params[\"b1\"])\n",
    "        \n",
    "        if epoch == 0:\n",
    "            dW1 = np.zeros_like(params[\"W1\"])\n",
    "            db1 = np.zeros_like(params[\"b1\"])\n",
    "            \n",
    "            dW2 = np.zeros_like(params[\"W2\"])\n",
    "            db2 = np.zeros_like(params[\"b2\"])\n",
    "            \n",
    "            dW3 = np.zeros_like(params[\"W3\"])\n",
    "            db3 = np.zeros_like(params[\"b3\"])\n",
    "        \n",
    "        print(\"epoch\", epoch)\n",
    "        for i in range (trainxs.shape[0]):\n",
    "            forward_res = forward(params, trainxs_shuffled[i])\n",
    "            ups = backprop(trainxs_shuffled[i], trainys_shuffled[i], params, forward_res)\n",
    "            grads = update_grads(grads, ups, trainxs_shuffled.shape[0])\n",
    "\n",
    "        params = update_params(grads, params, learning_rate, momentum, dW1, db1, dW2, db2, dW3, db3)\n",
    "        \n",
    "        loss_t = validation_loss_c(xs, ys, params)\n",
    "        loss_training.append(loss_t)\n",
    "        \n",
    "        accuracy_t = accuracy(xs, ys, params)\n",
    "        accuracy_training.append(accuracy_t)\n",
    "    \n",
    "        accuracy_valid = accuracy(devxs, devys, params)\n",
    "        accuracy_validation.append(accuracy_valid)\n",
    "        \n",
    "        loss_valid = validation_loss_c(devxs, devys, params)\n",
    "        print('validation loss', loss_valid)\n",
    "        print('want', log_loss(devys, [predict_raw(x, params) for x in devxs]))\n",
    "        print('training loss', loss_t)\n",
    "        print('training accuracy', accuracy_t)\n",
    "        if np.abs(loss_valid - loss_old) == 0:\n",
    "            row_epoch += 1\n",
    "            if row_epoch == 5:\n",
    "                break\n",
    "        else:\n",
    "            row_epoch = 0\n",
    "        loss_old = loss_valid\n",
    "        epoch += 1\n",
    "    return epoch, loss_training, accuracy_training, accuracy_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_mlp(x, params):    \n",
    "    forward_res = forward(params, x)\n",
    "    if forward_res[\"a3\"] >= 0.5:\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_raw(x, params):\n",
    "    forward_res = forward(params, x)\n",
    "    \n",
    "    return forward_res[\"a3\"][0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate accuracy\n",
    "def accuracy(devxs, devys, params):\n",
    "    accuracy = 0\n",
    "    for i in range (devxs.shape[0]):\n",
    "        yp = predict_mlp(devxs[i], params)\n",
    "        if (yp == devys[i]):\n",
    "            accuracy += 1\n",
    "    return (accuracy / devys.shape[0]) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to determine loss\n",
    "def validation_loss(devxs, devys, params):\n",
    "    l = 0\n",
    "    for i in range (devxs.shape[0]):\n",
    "        yprime = predict_mlp(devxs[i], params)\n",
    "        if (yprime == 0 and devys[i] == 1):\n",
    "            yprime += 1e-15\n",
    "        elif yprime == 1 and devys[i] == 0:\n",
    "            yprime -= 1e-15\n",
    "        if devys[i] == 1:\n",
    "            l += np.log(yprime)\n",
    "        else:\n",
    "            l += np.log(1 - yprime)\n",
    "    return ((-1) / devys.shape[0]) * l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CORRECT function to determine loss\n",
    "def validation_loss_c(devxs, devys, params):\n",
    "    l = 0\n",
    "    for i in range (devxs.shape[0]):\n",
    "        yprime = predict_raw(devxs[i], params)\n",
    "        if (yprime == 0 and devys[i] == 1):\n",
    "            yprime += 1e-15\n",
    "        elif yprime == 1 and devys[i] == 0:\n",
    "            yprime -= 1e-15\n",
    "        if devys[i] == 1:\n",
    "            l += np.log(yprime)\n",
    "        else:\n",
    "            l += np.log(1 - yprime)\n",
    "    return ((-1) / devys.shape[0]) * l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Training model to convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convergence is defined as no improvement in validation loss over 3 consecutive parameter updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameters\n",
    "\n",
    "# full-batch gradient descent, so not using batch_size as a parameter\n",
    "learning_rate = 0.1\n",
    "momentum = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_size = 28**2\n",
    "# h1 = 200\n",
    "# h2 = 20\n",
    "# out_size = 1\n",
    "\n",
    "# #     stores training set loss for part 3.5\n",
    "# loss_training = []\n",
    "\n",
    "# #     stores training set accuracy for part 3.6\n",
    "# accuracy_training = []\n",
    "\n",
    "# #     stores validation set accuracy for part 3.7\n",
    "# accuracy_validation = []\n",
    "\n",
    "# # initialise parameters\n",
    "# params = {\"W1\": np.random.randn(h1, img_size) * np.sqrt(1 / img_size),\n",
    "#           \"b1\": np.zeros((h1, 1)),\n",
    "#           \"W2\": np.random.randn(h2, h1) * np.sqrt(1 / h1),\n",
    "#           \"b2\": np.zeros((h2, 1)),\n",
    "#           \"W3\": np.random.randn(out_size, h2) * np.sqrt(1 / h2),\n",
    "#           \"b3\": np.zeros((out_size, 1))}\n",
    "# validation_loss(devxs, devys, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_old 0.7645963777269896\n",
      "epoch 0\n",
      "validation loss 0.7344045625833308\n",
      "want 0.7344045625833299\n",
      "training loss 0.7376115642031355\n",
      "training accuracy 50.0\n",
      "epoch 1\n",
      "validation loss 0.7165444121540523\n",
      "want 0.7165444121540531\n",
      "training loss 0.7190025228729319\n",
      "training accuracy 50.0\n",
      "epoch 2\n",
      "validation loss 0.7061413712555472\n",
      "want 0.7061413712555477\n",
      "training loss 0.7080361335061156\n",
      "training accuracy 50.0\n",
      "epoch 3\n",
      "validation loss 0.7000792481058767\n",
      "want 0.7000792481058766\n",
      "training loss 0.7015535667023478\n",
      "training accuracy 50.0\n",
      "epoch 4\n",
      "validation loss 0.6964845237629553\n",
      "want 0.6964845237629538\n",
      "training loss 0.6976466295767213\n",
      "training accuracy 50.0\n",
      "epoch 5\n",
      "validation loss 0.6942712760979594\n",
      "want 0.6942712760979594\n",
      "training loss 0.6952024440969852\n",
      "training accuracy 50.0\n",
      "epoch 6\n",
      "validation loss 0.6928233136870917\n",
      "want 0.6928233136870917\n",
      "training loss 0.6935843518206001\n",
      "training accuracy 50.0\n",
      "epoch 7\n",
      "validation loss 0.6917953617201196\n",
      "want 0.6917953617201194\n",
      "training loss 0.6924317138508421\n",
      "training accuracy 50.0\n",
      "epoch 8\n",
      "validation loss 0.6909955506403612\n",
      "want 0.6909955506403617\n",
      "training loss 0.6915411824806491\n",
      "training accuracy 50.0\n",
      "epoch 9\n",
      "validation loss 0.6903179788330687\n",
      "want 0.6903179788330688\n",
      "training loss 0.6907982934554265\n",
      "training accuracy 50.00833333333333\n",
      "epoch 10\n",
      "validation loss 0.6897046056133873\n",
      "want 0.6897046056133869\n",
      "training loss 0.6901386175867911\n",
      "training accuracy 50.041666666666664\n",
      "epoch 11\n",
      "validation loss 0.6891238964691921\n",
      "want 0.6891238964691923\n",
      "training loss 0.6895258503091031\n",
      "training accuracy 51.233333333333334\n",
      "epoch 12\n",
      "validation loss 0.6885589154113789\n",
      "want 0.6885589154113788\n",
      "training loss 0.6889394907039401\n",
      "training accuracy 54.666666666666664\n",
      "epoch 13\n",
      "validation loss 0.6880007101271113\n",
      "want 0.6880007101271105\n",
      "training loss 0.6883679198739707\n",
      "training accuracy 60.050000000000004\n",
      "epoch 14\n",
      "validation loss 0.6874446516345707\n",
      "want 0.6874446516345705\n",
      "training loss 0.6878045128039046\n",
      "training accuracy 64.85\n",
      "epoch 15\n",
      "validation loss 0.6868884188667426\n",
      "want 0.6868884188667409\n",
      "training loss 0.6872454535525664\n",
      "training accuracy 68.22500000000001\n",
      "epoch 16\n",
      "validation loss 0.6863308966426185\n",
      "want 0.6863308966426186\n",
      "training loss 0.6866885070406715\n",
      "training accuracy 70.09166666666667\n",
      "epoch 17\n",
      "validation loss 0.6857715790737849\n",
      "want 0.6857715790737838\n",
      "training loss 0.6861323283026648\n",
      "training accuracy 71.35833333333333\n",
      "epoch 18\n",
      "validation loss 0.685210251298567\n",
      "want 0.6852102512985677\n",
      "training loss 0.6855760738618051\n",
      "training accuracy 72.66666666666667\n",
      "epoch 19\n",
      "validation loss 0.6846468234056526\n",
      "want 0.684646823405653\n",
      "training loss 0.6850191830125122\n",
      "training accuracy 73.53333333333333\n",
      "epoch 20\n",
      "validation loss 0.6840812467262737\n",
      "want 0.6840812467262748\n",
      "training loss 0.6844612546890425\n",
      "training accuracy 74.19166666666666\n",
      "epoch 21\n",
      "validation loss 0.6835134740394887\n",
      "want 0.6835134740394898\n",
      "training loss 0.6839019781235224\n",
      "training accuracy 74.85000000000001\n",
      "epoch 22\n",
      "validation loss 0.682943442663134\n",
      "want 0.6829434426631339\n",
      "training loss 0.6833410937789085\n",
      "training accuracy 75.33333333333333\n",
      "epoch 23\n",
      "validation loss 0.6823710690531077\n",
      "want 0.682371069053108\n",
      "training loss 0.682778371324176\n",
      "training accuracy 75.64999999999999\n",
      "epoch 24\n",
      "validation loss 0.6817962488493798\n",
      "want 0.68179624884938\n",
      "training loss 0.6822135972040239\n",
      "training accuracy 76.2\n",
      "epoch 25\n",
      "validation loss 0.6812188592135068\n",
      "want 0.681218859213507\n",
      "training loss 0.6816465676108092\n",
      "training accuracy 76.575\n",
      "epoch 26\n",
      "validation loss 0.680638761874504\n",
      "want 0.6806387618745046\n",
      "training loss 0.6810770844987712\n",
      "training accuracy 76.95\n",
      "epoch 27\n",
      "validation loss 0.6800558061368276\n",
      "want 0.6800558061368276\n",
      "training loss 0.6805049533122091\n",
      "training accuracy 77.24166666666666\n",
      "epoch 28\n",
      "validation loss 0.6794698315393035\n",
      "want 0.6794698315393036\n",
      "training loss 0.6799299816797486\n",
      "training accuracy 77.47500000000001\n",
      "epoch 29\n",
      "validation loss 0.6788806700716887\n",
      "want 0.6788806700716896\n",
      "training loss 0.6793519786538053\n",
      "training accuracy 77.75\n",
      "epoch 30\n",
      "validation loss 0.6782881479577676\n",
      "want 0.6782881479577675\n",
      "training loss 0.678770754258125\n",
      "training accuracy 77.98333333333333\n",
      "epoch 31\n",
      "validation loss 0.6776920870561516\n",
      "want 0.6776920870561519\n",
      "training loss 0.6781861192101056\n",
      "training accuracy 78.14999999999999\n",
      "epoch 32\n",
      "validation loss 0.6770923059422588\n",
      "want 0.6770923059422587\n",
      "training loss 0.6775978847425465\n",
      "training accuracy 78.49166666666667\n",
      "epoch 33\n",
      "validation loss 0.6764886207331685\n",
      "want 0.6764886207331676\n",
      "training loss 0.677005862482452\n",
      "training accuracy 78.68333333333334\n",
      "epoch 34\n",
      "validation loss 0.6758808457096452\n",
      "want 0.6758808457096451\n",
      "training loss 0.6764098643630511\n",
      "training accuracy 78.91666666666667\n",
      "epoch 35\n",
      "validation loss 0.6752687937805374\n",
      "want 0.6752687937805367\n",
      "training loss 0.6758097025553206\n",
      "training accuracy 79.08333333333334\n",
      "epoch 36\n",
      "validation loss 0.6746522768259716\n",
      "want 0.6746522768259714\n",
      "training loss 0.6752051894114384\n",
      "training accuracy 79.24166666666666\n",
      "epoch 37\n",
      "validation loss 0.6740311059481531\n",
      "want 0.6740311059481534\n",
      "training loss 0.6745961374157698\n",
      "training accuracy 79.39166666666667\n",
      "epoch 38\n",
      "validation loss 0.6734050916521394\n",
      "want 0.6734050916521397\n",
      "training loss 0.673982359140803\n",
      "training accuracy 79.475\n",
      "epoch 39\n",
      "validation loss 0.6727740439738712\n",
      "want 0.672774043973871\n",
      "training loss 0.673363667206751\n",
      "training accuracy 79.58333333333333\n",
      "epoch 40\n",
      "validation loss 0.6721377725686755\n",
      "want 0.6721377725686748\n",
      "training loss 0.6727398742438072\n",
      "training accuracy 79.74166666666666\n",
      "epoch 41\n",
      "validation loss 0.6714960867703222\n",
      "want 0.6714960867703227\n",
      "training loss 0.6721107928567454\n",
      "training accuracy 79.9\n",
      "epoch 42\n",
      "validation loss 0.6708487956282988\n",
      "want 0.6708487956282977\n",
      "training loss 0.6714762355914637\n",
      "training accuracy 79.95833333333333\n",
      "epoch 43\n",
      "validation loss 0.6701957079290843\n",
      "want 0.6701957079290852\n",
      "training loss 0.6708360149034385\n",
      "training accuracy 80.05\n",
      "epoch 44\n",
      "validation loss 0.669536632205893\n",
      "want 0.6695366322058928\n",
      "training loss 0.670189943127947\n",
      "training accuracy 80.2\n",
      "epoch 45\n",
      "validation loss 0.6688713767401464\n",
      "want 0.6688713767401462\n",
      "training loss 0.669537832452112\n",
      "training accuracy 80.28333333333333\n",
      "epoch 46\n",
      "validation loss 0.6681997495573074\n",
      "want 0.6681997495573075\n",
      "training loss 0.6688794948887364\n",
      "training accuracy 80.36666666666666\n",
      "epoch 47\n",
      "validation loss 0.667521558418955\n",
      "want 0.667521558418955\n",
      "training loss 0.6682147422520262\n",
      "training accuracy 80.39166666666667\n",
      "epoch 48\n",
      "validation loss 0.6668366108126216\n",
      "want 0.6668366108126221\n",
      "training loss 0.6675433861352568\n",
      "training accuracy 80.44166666666666\n",
      "epoch 49\n",
      "validation loss 0.6661447139405466\n",
      "want 0.6661447139405462\n",
      "training loss 0.6668652378904594\n",
      "training accuracy 80.53333333333333\n",
      "epoch 50\n",
      "validation loss 0.6654456747082359\n",
      "want 0.6654456747082361\n",
      "training loss 0.6661801086102677\n",
      "training accuracy 80.63333333333334\n",
      "epoch 51\n",
      "validation loss 0.6647392997135714\n",
      "want 0.6647392997135714\n",
      "training loss 0.6654878091119761\n",
      "training accuracy 80.7\n",
      "epoch 52\n",
      "validation loss 0.6640253952370182\n",
      "want 0.6640253952370188\n",
      "training loss 0.6647881499240345\n",
      "training accuracy 80.78333333333333\n",
      "epoch 53\n",
      "validation loss 0.6633037672334391\n",
      "want 0.663303767233439\n",
      "training loss 0.6640809412750359\n",
      "training accuracy 80.85833333333333\n",
      "epoch 54\n",
      "validation loss 0.6625742213258929\n",
      "want 0.6625742213258938\n",
      "training loss 0.6633659930853836\n",
      "training accuracy 80.91666666666667\n",
      "epoch 55\n",
      "validation loss 0.6618365628018027\n",
      "want 0.6618365628018028\n",
      "training loss 0.6626431149617937\n",
      "training accuracy 81.0\n",
      "epoch 56\n",
      "validation loss 0.6610905966117616\n",
      "want 0.661090596611763\n",
      "training loss 0.6619121161948173\n",
      "training accuracy 81.05\n",
      "epoch 57\n",
      "validation loss 0.6603361273713216\n",
      "want 0.6603361273713203\n",
      "training loss 0.6611728057595421\n",
      "training accuracy 81.14166666666667\n",
      "epoch 58\n",
      "validation loss 0.6595729593659565\n",
      "want 0.6595729593659582\n",
      "training loss 0.6604249923196519\n",
      "training accuracy 81.22500000000001\n",
      "epoch 59\n",
      "validation loss 0.6588008965595621\n",
      "want 0.6588008965595621\n",
      "training loss 0.6596684842350721\n",
      "training accuracy 81.31666666666668\n",
      "epoch 60\n",
      "validation loss 0.6580197426066096\n",
      "want 0.6580197426066089\n",
      "training loss 0.6589030895733922\n",
      "training accuracy 81.40833333333333\n",
      "epoch 61\n",
      "validation loss 0.6572293008683225\n",
      "want 0.657229300868322\n",
      "training loss 0.6581286161252377\n",
      "training accuracy 81.45\n",
      "epoch 62\n",
      "validation loss 0.6564293744330376\n",
      "want 0.656429374433037\n",
      "training loss 0.6573448714238924\n",
      "training accuracy 81.53333333333333\n",
      "epoch 63\n",
      "validation loss 0.655619766141018\n",
      "want 0.6556197661410179\n",
      "training loss 0.6565516627692711\n",
      "training accuracy 81.6\n",
      "epoch 64\n",
      "validation loss 0.6548002786139641\n",
      "want 0.6548002786139638\n",
      "training loss 0.6557487972565936\n",
      "training accuracy 81.60833333333333\n",
      "epoch 65\n",
      "validation loss 0.6539707142894501\n",
      "want 0.6539707142894488\n",
      "training loss 0.6549360818098912\n",
      "training accuracy 81.65\n",
      "epoch 66\n",
      "validation loss 0.6531308754605331\n",
      "want 0.653130875460534\n",
      "training loss 0.6541133232206611\n",
      "training accuracy 81.68333333333334\n",
      "epoch 67\n",
      "validation loss 0.6522805643207981\n",
      "want 0.6522805643207983\n",
      "training loss 0.6532803281918073\n",
      "training accuracy 81.72500000000001\n",
      "epoch 68\n",
      "validation loss 0.6514195830150258\n",
      "want 0.6514195830150247\n",
      "training loss 0.6524369033872364\n",
      "training accuracy 81.78333333333333\n",
      "epoch 69\n",
      "validation loss 0.6505477336957858\n",
      "want 0.6505477336957859\n",
      "training loss 0.6515828554871984\n",
      "training accuracy 81.80833333333334\n",
      "epoch 70\n",
      "validation loss 0.649664818586167\n",
      "want 0.649664818586167\n",
      "training loss 0.6507179912497113\n",
      "training accuracy 81.83333333333334\n",
      "epoch 71\n",
      "validation loss 0.6487706400488643\n",
      "want 0.6487706400488635\n",
      "training loss 0.6498421175783067\n",
      "training accuracy 81.91666666666667\n",
      "epoch 72\n",
      "validation loss 0.6478650006618852\n",
      "want 0.6478650006618852\n",
      "training loss 0.6489550415962259\n",
      "training accuracy 81.94166666666666\n",
      "epoch 73\n",
      "validation loss 0.6469477033010944\n",
      "want 0.6469477033010945\n",
      "training loss 0.6480565707274367\n",
      "training accuracy 82.00833333333334\n",
      "epoch 74\n",
      "validation loss 0.6460185512298036\n",
      "want 0.6460185512298028\n",
      "training loss 0.6471465127845651\n",
      "training accuracy 82.075\n",
      "epoch 75\n",
      "validation loss 0.6450773481956366\n",
      "want 0.6450773481956362\n",
      "training loss 0.6462246760640726\n",
      "training accuracy 82.14166666666667\n",
      "epoch 76\n",
      "validation loss 0.644123898534878\n",
      "want 0.6441238985348771\n",
      "training loss 0.6452908694487421\n",
      "training accuracy 82.18333333333334\n",
      "epoch 77\n",
      "validation loss 0.6431580072844776\n",
      "want 0.643158007284477\n",
      "training loss 0.6443449025178407\n",
      "training accuracy 82.25\n",
      "epoch 78\n",
      "validation loss 0.6421794803019202\n",
      "want 0.64217948030192\n",
      "training loss 0.6433865856650046\n",
      "training accuracy 82.30833333333332\n",
      "epoch 79\n",
      "validation loss 0.6411881243931105\n",
      "want 0.6411881243931106\n",
      "training loss 0.6424157302240638\n",
      "training accuracy 82.33333333333334\n",
      "epoch 80\n",
      "validation loss 0.6401837474484325\n",
      "want 0.6401837474484321\n",
      "training loss 0.6414321486029924\n",
      "training accuracy 82.39999999999999\n",
      "epoch 81\n",
      "validation loss 0.6391661585871169\n",
      "want 0.6391661585871166\n",
      "training loss 0.6404356544260748\n",
      "training accuracy 82.48333333333333\n",
      "epoch 82\n",
      "validation loss 0.6381351683100404\n",
      "want 0.6381351683100406\n",
      "training loss 0.6394260626844556\n",
      "training accuracy 82.55833333333334\n",
      "epoch 83\n",
      "validation loss 0.6370905886610393\n",
      "want 0.637090588661039\n",
      "training loss 0.6384031898950866\n",
      "training accuracy 82.60833333333333\n",
      "epoch 84\n",
      "validation loss 0.6360322333968144\n",
      "want 0.6360322333968143\n",
      "training loss 0.6373668542682772\n",
      "training accuracy 82.66666666666667\n",
      "epoch 85\n",
      "validation loss 0.6349599181654798\n",
      "want 0.6349599181654806\n",
      "training loss 0.6363168758837864\n",
      "training accuracy 82.71666666666667\n",
      "epoch 86\n",
      "validation loss 0.633873460693768\n",
      "want 0.6338734606937683\n",
      "training loss 0.6352530768754983\n",
      "training accuracy 82.75833333333334\n",
      "epoch 87\n",
      "validation loss 0.632772680982875\n",
      "want 0.6327726809828753\n",
      "training loss 0.6341752816247617\n",
      "training accuracy 82.78333333333333\n",
      "epoch 88\n",
      "validation loss 0.6316574015129237\n",
      "want 0.6316574015129233\n",
      "training loss 0.6330833169622355\n",
      "training accuracy 82.81666666666668\n",
      "epoch 89\n",
      "validation loss 0.6305274474559461\n",
      "want 0.6305274474559456\n",
      "training loss 0.6319770123782577\n",
      "training accuracy 82.86666666666666\n",
      "epoch 90\n",
      "validation loss 0.6293826468972976\n",
      "want 0.6293826468972976\n",
      "training loss 0.630856200241584\n",
      "training accuracy 82.90833333333333\n",
      "epoch 91\n",
      "validation loss 0.6282228310653362\n",
      "want 0.6282228310653365\n",
      "training loss 0.6297207160264247\n",
      "training accuracy 82.975\n",
      "epoch 92\n",
      "validation loss 0.6270478345691914\n",
      "want 0.627047834569192\n",
      "training loss 0.6285703985474879\n",
      "training accuracy 82.99166666666666\n",
      "epoch 93\n",
      "validation loss 0.6258574956443901\n",
      "want 0.6258574956443902\n",
      "training loss 0.6274050902029278\n",
      "training accuracy 83.01666666666667\n",
      "epoch 94\n",
      "validation loss 0.6246516564060673\n",
      "want 0.6246516564060678\n",
      "training loss 0.6262246372248407\n",
      "training accuracy 83.06666666666666\n",
      "epoch 95\n",
      "validation loss 0.6234301631094552\n",
      "want 0.623430163109456\n",
      "training loss 0.6250288899370728\n",
      "training accuracy 83.10833333333333\n",
      "epoch 96\n",
      "validation loss 0.6221928664172738\n",
      "want 0.6221928664172741\n",
      "training loss 0.62381770301993\n",
      "training accuracy 83.09166666666667\n",
      "epoch 97\n",
      "validation loss 0.620939621673624\n",
      "want 0.6209396216736246\n",
      "training loss 0.6225909357814351\n",
      "training accuracy 83.10833333333333\n",
      "epoch 98\n",
      "validation loss 0.6196702891839302\n",
      "want 0.6196702891839294\n",
      "training loss 0.6213484524346483\n",
      "training accuracy 83.15833333333333\n",
      "epoch 99\n",
      "validation loss 0.6183847345004039\n",
      "want 0.6183847345004045\n",
      "training loss 0.6200901223805788\n",
      "training accuracy 83.24166666666667\n",
      "epoch 100\n",
      "validation loss 0.6170828287125131\n",
      "want 0.6170828287125124\n",
      "training loss 0.6188158204961471\n",
      "training accuracy 83.29166666666666\n",
      "epoch 101\n",
      "validation loss 0.6157644487417876\n",
      "want 0.6157644487417869\n",
      "training loss 0.6175254274265615\n",
      "training accuracy 83.31666666666668\n",
      "epoch 102\n",
      "validation loss 0.6144294776403699\n",
      "want 0.6144294776403707\n",
      "training loss 0.6162188298815005\n",
      "training accuracy 83.375\n",
      "epoch 103\n",
      "validation loss 0.6130778048925594\n",
      "want 0.6130778048925591\n",
      "training loss 0.6148959209343984\n",
      "training accuracy 83.44166666666666\n",
      "epoch 104\n",
      "validation loss 0.6117093267185894\n",
      "want 0.6117093267185889\n",
      "training loss 0.6135566003240683\n",
      "training accuracy 83.48333333333333\n",
      "epoch 105\n",
      "validation loss 0.6103239463798674\n",
      "want 0.6103239463798675\n",
      "training loss 0.6122007747578908\n",
      "training accuracy 83.55833333333334\n",
      "epoch 106\n",
      "validation loss 0.6089215744847823\n",
      "want 0.6089215744847833\n",
      "training loss 0.610828358215705\n",
      "training accuracy 83.575\n",
      "epoch 107\n",
      "validation loss 0.6075021292942008\n",
      "want 0.6075021292942003\n",
      "training loss 0.6094392722535159\n",
      "training accuracy 83.59166666666667\n",
      "epoch 108\n",
      "validation loss 0.6060655370256869\n",
      "want 0.6060655370256868\n",
      "training loss 0.6080334463061144\n",
      "training accuracy 83.59166666666667\n",
      "epoch 109\n",
      "validation loss 0.6046117321554937\n",
      "want 0.6046117321554929\n",
      "training loss 0.6066108179875688\n",
      "training accuracy 83.61666666666666\n",
      "epoch 110\n",
      "validation loss 0.6031406577172502\n",
      "want 0.60314065771725\n",
      "training loss 0.6051713333886839\n",
      "training accuracy 83.63333333333334\n",
      "epoch 111\n",
      "validation loss 0.6016522655963297\n",
      "want 0.6016522655963301\n",
      "training loss 0.6037149473702137\n",
      "training accuracy 83.69166666666666\n",
      "epoch 112\n",
      "validation loss 0.600146516818771\n",
      "want 0.6001465168187712\n",
      "training loss 0.6022416238509217\n",
      "training accuracy 83.74166666666667\n",
      "epoch 113\n",
      "validation loss 0.5986233818336468\n",
      "want 0.598623381833646\n",
      "training loss 0.6007513360892709\n",
      "training accuracy 83.76666666666667\n",
      "epoch 114\n",
      "validation loss 0.5970828407877304\n",
      "want 0.5970828407877309\n",
      "training loss 0.5992440669576216\n",
      "training accuracy 83.8\n",
      "epoch 115\n",
      "validation loss 0.5955248837913104\n",
      "want 0.5955248837913097\n",
      "training loss 0.5977198092078273\n",
      "training accuracy 83.8\n",
      "epoch 116\n",
      "validation loss 0.5939495111739382\n",
      "want 0.5939495111739388\n",
      "training loss 0.5961785657269971\n",
      "training accuracy 83.81666666666666\n",
      "epoch 117\n",
      "validation loss 0.5923567337289869\n",
      "want 0.5923567337289855\n",
      "training loss 0.5946203497823337\n",
      "training accuracy 83.875\n",
      "epoch 118\n",
      "validation loss 0.5907465729457583\n",
      "want 0.5907465729457585\n",
      "training loss 0.5930451852537814\n",
      "training accuracy 83.89999999999999\n",
      "epoch 119\n",
      "validation loss 0.5891190612280485\n",
      "want 0.5891190612280488\n",
      "training loss 0.591453106853364\n",
      "training accuracy 83.91666666666666\n",
      "epoch 120\n",
      "validation loss 0.5874742420979101\n",
      "want 0.5874742420979099\n",
      "training loss 0.5898441603300579\n",
      "training accuracy 83.98333333333333\n",
      "epoch 121\n",
      "validation loss 0.5858121703835294\n",
      "want 0.5858121703835292\n",
      "training loss 0.5882184026589959\n",
      "training accuracy 84.04166666666667\n",
      "epoch 122\n",
      "validation loss 0.5841329123900624\n",
      "want 0.5841329123900626\n",
      "training loss 0.5865759022139445\n",
      "training accuracy 84.05833333333334\n",
      "epoch 123\n",
      "validation loss 0.582436546052339\n",
      "want 0.5824365460523377\n",
      "training loss 0.5849167389219114\n",
      "training accuracy 84.08333333333333\n",
      "epoch 124\n",
      "validation loss 0.5807231610683715\n",
      "want 0.5807231610683715\n",
      "training loss 0.5832410043988668\n",
      "training accuracy 84.14166666666667\n",
      "epoch 125\n",
      "validation loss 0.5789928590126924\n",
      "want 0.5789928590126919\n",
      "training loss 0.5815488020655255\n",
      "training accuracy 84.18333333333334\n",
      "epoch 126\n",
      "validation loss 0.5772457534285083\n",
      "want 0.5772457534285084\n",
      "training loss 0.5798402472422662\n",
      "training accuracy 84.22500000000001\n",
      "epoch 127\n",
      "validation loss 0.5754819698978327\n",
      "want 0.5754819698978333\n",
      "training loss 0.5781154672222693\n",
      "training accuracy 84.275\n",
      "epoch 128\n",
      "validation loss 0.5737016460887263\n",
      "want 0.573701646088727\n",
      "training loss 0.57637460132201\n",
      "training accuracy 84.29166666666667\n",
      "epoch 129\n",
      "validation loss 0.5719049317789092\n",
      "want 0.5719049317789086\n",
      "training loss 0.5746178009084152\n",
      "training accuracy 84.325\n",
      "epoch 130\n",
      "validation loss 0.5700919888550577\n",
      "want 0.5700919888550582\n",
      "training loss 0.5728452294018829\n",
      "training accuracy 84.36666666666667\n",
      "epoch 131\n",
      "validation loss 0.5682629912872168\n",
      "want 0.5682629912872167\n",
      "training loss 0.5710570622546641\n",
      "training accuracy 84.43333333333334\n",
      "epoch 132\n",
      "validation loss 0.5664181250777872\n",
      "want 0.5664181250777884\n",
      "training loss 0.5692534869040108\n",
      "training accuracy 84.44166666666668\n",
      "epoch 133\n",
      "validation loss 0.5645575881847381\n",
      "want 0.5645575881847382\n",
      "training loss 0.5674347026997227\n",
      "training accuracy 84.46666666666667\n",
      "epoch 134\n",
      "validation loss 0.5626815904186862\n",
      "want 0.5626815904186859\n",
      "training loss 0.5656009208057279\n",
      "training accuracy 84.55833333333334\n",
      "epoch 135\n",
      "validation loss 0.5607903533136956\n",
      "want 0.5607903533136953\n",
      "training loss 0.5637523640754766\n",
      "training accuracy 84.6\n",
      "epoch 136\n",
      "validation loss 0.5588841099716751\n",
      "want 0.5588841099716751\n",
      "training loss 0.5618892669010562\n",
      "training accuracy 84.64166666666667\n",
      "epoch 137\n",
      "validation loss 0.5569631048804099\n",
      "want 0.5569631048804108\n",
      "training loss 0.5600118750360172\n",
      "training accuracy 84.66666666666667\n",
      "epoch 138\n",
      "validation loss 0.5550275937053625\n",
      "want 0.5550275937053629\n",
      "training loss 0.558120445391949\n",
      "training accuracy 84.7\n",
      "epoch 139\n",
      "validation loss 0.5530778430554862\n",
      "want 0.5530778430554858\n",
      "training loss 0.5562152458091222\n",
      "training accuracy 84.7\n",
      "epoch 140\n",
      "validation loss 0.5511141302234316\n",
      "want 0.5511141302234309\n",
      "training loss 0.5542965548014572\n",
      "training accuracy 84.71666666666667\n",
      "epoch 141\n",
      "validation loss 0.5491367429006178\n",
      "want 0.5491367429006184\n",
      "training loss 0.5523646612762562\n",
      "training accuracy 84.78333333333333\n",
      "epoch 142\n",
      "validation loss 0.5471459788677732\n",
      "want 0.5471459788677737\n",
      "training loss 0.5504198642292999\n",
      "training accuracy 84.88333333333333\n",
      "epoch 143\n",
      "validation loss 0.5451421456616384\n",
      "want 0.5451421456616391\n",
      "training loss 0.5484624724159646\n",
      "training accuracy 84.90833333333333\n",
      "epoch 144\n",
      "validation loss 0.5431255602186813\n",
      "want 0.543125560218682\n",
      "training loss 0.5464928039990776\n",
      "training accuracy 84.925\n",
      "epoch 145\n",
      "validation loss 0.541096548496728\n",
      "want 0.5410965484967281\n",
      "training loss 0.5445111861744896\n",
      "training accuracy 84.95833333333334\n",
      "epoch 146\n",
      "validation loss 0.5390554450755543\n",
      "want 0.5390554450755539\n",
      "training loss 0.5425179547752615\n",
      "training accuracy 85.0\n",
      "epoch 147\n",
      "validation loss 0.5370025927375688\n",
      "want 0.5370025927375692\n",
      "training loss 0.5405134538556491\n",
      "training accuracy 85.08333333333333\n",
      "epoch 148\n",
      "validation loss 0.5349383420298154\n",
      "want 0.5349383420298152\n",
      "training loss 0.5384980352559235\n",
      "training accuracy 85.11666666666666\n",
      "epoch 149\n",
      "validation loss 0.5328630508085951\n",
      "want 0.532863050808595\n",
      "training loss 0.536472058149468\n",
      "training accuracy 85.19166666666666\n",
      "epoch 150\n",
      "validation loss 0.5307770837681287\n",
      "want 0.5307770837681287\n",
      "training loss 0.5344358885733524\n",
      "training accuracy 85.26666666666667\n",
      "epoch 151\n",
      "validation loss 0.5286808119547068\n",
      "want 0.5286808119547066\n",
      "training loss 0.5323898989439108\n",
      "training accuracy 85.3\n",
      "epoch 152\n",
      "validation loss 0.526574612267872\n",
      "want 0.5265746122678723\n",
      "training loss 0.5303344675587568\n",
      "training accuracy 85.35000000000001\n",
      "epoch 153\n",
      "validation loss 0.5244588669502352\n",
      "want 0.5244588669502352\n",
      "training loss 0.5282699780868233\n",
      "training accuracy 85.39166666666667\n",
      "epoch 154\n",
      "validation loss 0.522333963067554\n",
      "want 0.5223339630675541\n",
      "training loss 0.526196819048028\n",
      "training accuracy 85.40833333333333\n",
      "epoch 155\n",
      "validation loss 0.5202002919807776\n",
      "want 0.5202002919807772\n",
      "training loss 0.5241153832841888\n",
      "training accuracy 85.44166666666668\n",
      "epoch 156\n",
      "validation loss 0.5180582488117583\n",
      "want 0.5180582488117584\n",
      "training loss 0.5220260674229251\n",
      "training accuracy 85.49166666666666\n",
      "epoch 157\n",
      "validation loss 0.5159082319043863\n",
      "want 0.5159082319043863\n",
      "training loss 0.5199292713362047\n",
      "training accuracy 85.54166666666667\n",
      "epoch 158\n",
      "validation loss 0.5137506422828813\n",
      "want 0.5137506422828813\n",
      "training loss 0.5178253975953154\n",
      "training accuracy 85.6\n",
      "epoch 159\n",
      "validation loss 0.5115858831090158\n",
      "want 0.5115858831090151\n",
      "training loss 0.5157148509239319\n",
      "training accuracy 85.625\n",
      "epoch 160\n",
      "validation loss 0.5094143591400041\n",
      "want 0.5094143591400043\n",
      "training loss 0.5135980376510828\n",
      "training accuracy 85.65833333333333\n",
      "epoch 161\n",
      "validation loss 0.5072364761888112\n",
      "want 0.5072364761888108\n",
      "training loss 0.5114753651657064\n",
      "training accuracy 85.68333333333334\n",
      "epoch 162\n",
      "validation loss 0.5050526405885609\n",
      "want 0.5050526405885613\n",
      "training loss 0.5093472413744967\n",
      "training accuracy 85.69166666666666\n",
      "epoch 163\n",
      "validation loss 0.5028632586627614\n",
      "want 0.5028632586627614\n",
      "training loss 0.5072140741647213\n",
      "training accuracy 85.75\n",
      "epoch 164\n",
      "validation loss 0.5006687362029395\n",
      "want 0.5006687362029394\n",
      "training loss 0.5050762708736882\n",
      "training accuracy 85.80833333333334\n",
      "epoch 165\n",
      "validation loss 0.49846947795530383\n",
      "want 0.4984694779553037\n",
      "training loss 0.5029342377663714\n",
      "training accuracy 85.83333333333333\n",
      "epoch 166\n",
      "validation loss 0.4962658871179412\n",
      "want 0.49626588711794134\n",
      "training loss 0.5007883795228532\n",
      "training accuracy 85.86666666666667\n",
      "epoch 167\n",
      "validation loss 0.4940583648500177\n",
      "want 0.4940583648500175\n",
      "training loss 0.49863909873697326\n",
      "training accuracy 85.89166666666667\n",
      "epoch 168\n",
      "validation loss 0.49184730979436536\n",
      "want 0.4918473097943653\n",
      "training loss 0.49648679542763413\n",
      "training accuracy 85.95\n",
      "epoch 169\n",
      "validation loss 0.48963311761477796\n",
      "want 0.4896331176147778\n",
      "training loss 0.49433186656414885\n",
      "training accuracy 85.95833333333334\n",
      "epoch 170\n",
      "validation loss 0.4874161805492276\n",
      "want 0.487416180549228\n",
      "training loss 0.4921747056068259\n",
      "training accuracy 85.98333333333333\n",
      "epoch 171\n",
      "validation loss 0.4851968869801558\n",
      "want 0.48519688698015595\n",
      "training loss 0.49001570206402734\n",
      "training accuracy 86.02499999999999\n",
      "epoch 172\n",
      "validation loss 0.4829756210228702\n",
      "want 0.48297562102287\n",
      "training loss 0.4878552410668066\n",
      "training accuracy 86.06666666666666\n",
      "epoch 173\n",
      "validation loss 0.4807527621330099\n",
      "want 0.4807527621330095\n",
      "training loss 0.48569370296206776\n",
      "training accuracy 86.13333333333333\n",
      "epoch 174\n",
      "validation loss 0.4785286847339209\n",
      "want 0.4785286847339216\n",
      "training loss 0.48353146292523\n",
      "training accuracy 86.15833333333333\n",
      "epoch 175\n",
      "validation loss 0.4763037578647015\n",
      "want 0.4763037578647013\n",
      "training loss 0.48136889059317906\n",
      "training accuracy 86.20833333333333\n",
      "epoch 176\n",
      "validation loss 0.47407834484953865\n",
      "want 0.47407834484953926\n",
      "training loss 0.479206349718207\n",
      "training accuracy 86.28333333333333\n",
      "epoch 177\n",
      "validation loss 0.47185280298892424\n",
      "want 0.47185280298892374\n",
      "training loss 0.4770441978435984\n",
      "training accuracy 86.325\n",
      "epoch 178\n",
      "validation loss 0.46962748327313447\n",
      "want 0.4696274832731345\n",
      "training loss 0.47488278600131895\n",
      "training accuracy 86.38333333333334\n",
      "epoch 179\n",
      "validation loss 0.46740273011836564\n",
      "want 0.46740273011836575\n",
      "training loss 0.47272245843228733\n",
      "training accuracy 86.46666666666667\n",
      "epoch 180\n",
      "validation loss 0.46517888112571393\n",
      "want 0.46517888112571365\n",
      "training loss 0.47056355232949454\n",
      "training accuracy 86.51666666666667\n",
      "epoch 181\n",
      "validation loss 0.4629562668631627\n",
      "want 0.46295626686316266\n",
      "training loss 0.4684063976041923\n",
      "training accuracy 86.575\n",
      "epoch 182\n",
      "validation loss 0.4607352106706092\n",
      "want 0.4607352106706088\n",
      "training loss 0.4662513166753136\n",
      "training accuracy 86.70833333333333\n",
      "epoch 183\n",
      "validation loss 0.4585160284878604\n",
      "want 0.4585160284878602\n",
      "training loss 0.4640986242820609\n",
      "training accuracy 86.75\n",
      "epoch 184\n",
      "validation loss 0.45629902870546685\n",
      "want 0.45629902870546707\n",
      "training loss 0.46194862731969344\n",
      "training accuracy 86.79166666666667\n",
      "epoch 185\n",
      "validation loss 0.454084512038142\n",
      "want 0.454084512038142\n",
      "training loss 0.4598016246982906\n",
      "training accuracy 86.85000000000001\n",
      "epoch 186\n",
      "validation loss 0.45187277142044885\n",
      "want 0.451872771420449\n",
      "training loss 0.4576579072242401\n",
      "training accuracy 86.88333333333334\n",
      "epoch 187\n",
      "validation loss 0.44966409192435997\n",
      "want 0.4496640919243597\n",
      "training loss 0.4555177575041959\n",
      "training accuracy 86.96666666666667\n",
      "epoch 188\n",
      "validation loss 0.44745875069819985\n",
      "want 0.4474587506981999\n",
      "training loss 0.45338144987099765\n",
      "training accuracy 86.97500000000001\n",
      "epoch 189\n",
      "validation loss 0.4452570169264397\n",
      "want 0.4452570169264399\n",
      "training loss 0.4512492503311774\n",
      "training accuracy 87.02499999999999\n",
      "epoch 190\n",
      "validation loss 0.443059151809717\n",
      "want 0.4430591518097165\n",
      "training loss 0.44912141653342924\n",
      "training accuracy 87.05833333333334\n",
      "epoch 191\n",
      "validation loss 0.44086540856441636\n",
      "want 0.44086540856441586\n",
      "training loss 0.44699819775747995\n",
      "training accuracy 87.13333333333333\n",
      "epoch 192\n",
      "validation loss 0.43867603244108966\n",
      "want 0.4386760324410902\n",
      "training loss 0.44487983492265665\n",
      "training accuracy 87.20833333333333\n",
      "epoch 193\n",
      "validation loss 0.43649126076093564\n",
      "want 0.43649126076093514\n",
      "training loss 0.4427665606154774\n",
      "training accuracy 87.24166666666666\n",
      "epoch 194\n",
      "validation loss 0.4343113229695083\n",
      "want 0.43431132296950803\n",
      "training loss 0.440658599135418\n",
      "training accuracy 87.31666666666666\n",
      "epoch 195\n",
      "validation loss 0.4321364407068325\n",
      "want 0.43213644070683255\n",
      "training loss 0.43855616655811136\n",
      "training accuracy 87.33333333333333\n",
      "epoch 196\n",
      "validation loss 0.4299668278930004\n",
      "want 0.42996682789300034\n",
      "training loss 0.43645947081512015\n",
      "training accuracy 87.36666666666667\n",
      "epoch 197\n",
      "validation loss 0.427802690828354\n",
      "want 0.42780269082835365\n",
      "training loss 0.43436871178930975\n",
      "training accuracy 87.38333333333334\n",
      "epoch 198\n",
      "validation loss 0.42564422830731324\n",
      "want 0.42564422830731313\n",
      "training loss 0.43228408142506963\n",
      "training accuracy 87.46666666666667\n",
      "epoch 199\n",
      "validation loss 0.42349163174489474\n",
      "want 0.423491631744895\n",
      "training loss 0.43020576385223314\n",
      "training accuracy 87.49166666666667\n",
      "epoch 200\n",
      "validation loss 0.4213450853149526\n",
      "want 0.42134508531495257\n",
      "training loss 0.42813393552299106\n",
      "training accuracy 87.575\n",
      "epoch 201\n",
      "validation loss 0.4192047660991697\n",
      "want 0.41920476609916996\n",
      "training loss 0.4260687653605852\n",
      "training accuracy 87.6\n",
      "epoch 202\n",
      "validation loss 0.417070844245832\n",
      "want 0.41707084424583196\n",
      "training loss 0.42401041491901353\n",
      "training accuracy 87.64166666666667\n",
      "epoch 203\n",
      "validation loss 0.4149434831373991\n",
      "want 0.41494348313739876\n",
      "training loss 0.42195903855260675\n",
      "training accuracy 87.69166666666666\n",
      "epoch 204\n",
      "validation loss 0.4128228395659192\n",
      "want 0.4128228395659187\n",
      "training loss 0.4199147835946289\n",
      "training accuracy 87.69166666666666\n",
      "epoch 205\n",
      "validation loss 0.41070906391532275\n",
      "want 0.41070906391532286\n",
      "training loss 0.4178777905438502\n",
      "training accuracy 87.75833333333334\n",
      "epoch 206\n",
      "validation loss 0.40860230034965994\n",
      "want 0.40860230034966005\n",
      "training loss 0.41584819325811956\n",
      "training accuracy 87.81666666666666\n",
      "epoch 207\n",
      "validation loss 0.4065026870063487\n",
      "want 0.4065026870063489\n",
      "training loss 0.4138261191540787\n",
      "training accuracy 87.85833333333333\n",
      "epoch 208\n",
      "validation loss 0.4044103561935416\n",
      "want 0.40441035619354165\n",
      "training loss 0.41181168941196356\n",
      "training accuracy 87.90833333333333\n",
      "epoch 209\n",
      "validation loss 0.4023254345907224\n",
      "want 0.4023254345907221\n",
      "training loss 0.4098050191846758\n",
      "training accuracy 87.96666666666667\n",
      "epoch 210\n",
      "validation loss 0.400248043451683\n",
      "want 0.40024804345168347\n",
      "training loss 0.4078062178101763\n",
      "training accuracy 88.06666666666668\n",
      "epoch 211\n",
      "validation loss 0.39817829880906086\n",
      "want 0.3981782988090611\n",
      "training loss 0.40581538902633063\n",
      "training accuracy 88.125\n",
      "epoch 212\n",
      "validation loss 0.3961163116796281\n",
      "want 0.39611631167962824\n",
      "training loss 0.40383263118742607\n",
      "training accuracy 88.175\n",
      "epoch 213\n",
      "validation loss 0.3940621882695941\n",
      "want 0.3940621882695942\n",
      "training loss 0.40185803748148263\n",
      "training accuracy 88.24166666666666\n",
      "epoch 214\n",
      "validation loss 0.39201603017917774\n",
      "want 0.3920160301791782\n",
      "training loss 0.3998916961476049\n",
      "training accuracy 88.26666666666667\n",
      "epoch 215\n",
      "validation loss 0.38997793460577046\n",
      "want 0.3899779346057701\n",
      "training loss 0.39793369069266576\n",
      "training accuracy 88.29166666666667\n",
      "epoch 216\n",
      "validation loss 0.38794799454502654\n",
      "want 0.3879479945450272\n",
      "training loss 0.3959841001065513\n",
      "training accuracy 88.325\n",
      "epoch 217\n",
      "validation loss 0.38592629898928926\n",
      "want 0.38592629898928954\n",
      "training loss 0.3940429990753563\n",
      "training accuracy 88.43333333333334\n",
      "epoch 218\n",
      "validation loss 0.38391293312274366\n",
      "want 0.3839129331227433\n",
      "training loss 0.39211045819185325\n",
      "training accuracy 88.46666666666667\n",
      "epoch 219\n",
      "validation loss 0.3819079785127932\n",
      "want 0.3819079785127933\n",
      "training loss 0.39018654416267645\n",
      "training accuracy 88.56666666666668\n",
      "epoch 220\n",
      "validation loss 0.3799115132971506\n",
      "want 0.3799115132971509\n",
      "training loss 0.38827132001165016\n",
      "training accuracy 88.61666666666666\n",
      "epoch 221\n",
      "validation loss 0.37792361236618205\n",
      "want 0.37792361236618194\n",
      "training loss 0.38636484527873344\n",
      "training accuracy 88.64166666666667\n",
      "epoch 222\n",
      "validation loss 0.37594434754009826\n",
      "want 0.37594434754009853\n",
      "training loss 0.38446717621416404\n",
      "training accuracy 88.68333333333334\n",
      "epoch 223\n",
      "validation loss 0.37397378774061957\n",
      "want 0.37397378774061996\n",
      "training loss 0.38257836596727707\n",
      "training accuracy 88.75\n",
      "epoch 224\n",
      "validation loss 0.37201199915676664\n",
      "want 0.37201199915676664\n",
      "training loss 0.3806984647697206\n",
      "training accuracy 88.80833333333334\n",
      "epoch 225\n",
      "validation loss 0.3700590454044892\n",
      "want 0.3700590454044892\n",
      "training loss 0.3788275201125979\n",
      "training accuracy 88.85833333333333\n",
      "epoch 226\n",
      "validation loss 0.36811498767987477\n",
      "want 0.36811498767987516\n",
      "training loss 0.3769655769173509\n",
      "training accuracy 88.9\n",
      "epoch 227\n",
      "validation loss 0.36617988490571063\n",
      "want 0.3661798849057105\n",
      "training loss 0.3751126776999753\n",
      "training accuracy 88.925\n",
      "epoch 228\n",
      "validation loss 0.3642537938712131\n",
      "want 0.36425379387121337\n",
      "training loss 0.3732688627284704\n",
      "training accuracy 89.01666666666667\n",
      "epoch 229\n",
      "validation loss 0.36233676936478854\n",
      "want 0.3623367693647883\n",
      "training loss 0.3714341701732075\n",
      "training accuracy 89.05\n",
      "epoch 230\n",
      "validation loss 0.36042886429969045\n",
      "want 0.36042886429969023\n",
      "training loss 0.3696086362501623\n",
      "training accuracy 89.05833333333332\n",
      "epoch 231\n",
      "validation loss 0.3585301298325126\n",
      "want 0.35853012983251303\n",
      "training loss 0.3677922953567954\n",
      "training accuracy 89.075\n",
      "epoch 232\n",
      "validation loss 0.35664061547445675\n",
      "want 0.35664061547445647\n",
      "training loss 0.3659851802005722\n",
      "training accuracy 89.19166666666666\n",
      "epoch 233\n",
      "validation loss 0.35476036919535325\n",
      "want 0.3547603691953532\n",
      "training loss 0.3641873219200022\n",
      "training accuracy 89.28333333333333\n",
      "epoch 234\n",
      "validation loss 0.35288943752046553\n",
      "want 0.35288943752046487\n",
      "training loss 0.3623987501982178\n",
      "training accuracy 89.35\n",
      "epoch 235\n",
      "validation loss 0.3510278656200891\n",
      "want 0.35102786562008914\n",
      "training loss 0.3606194933690947\n",
      "training accuracy 89.4\n",
      "epoch 236\n",
      "validation loss 0.34917569739204163\n",
      "want 0.349175697392041\n",
      "training loss 0.3588495785159079\n",
      "training accuracy 89.45\n",
      "epoch 237\n",
      "validation loss 0.3473329755371002\n",
      "want 0.3473329755371003\n",
      "training loss 0.35708903156265354\n",
      "training accuracy 89.53333333333333\n",
      "epoch 238\n",
      "validation loss 0.34549974162753905\n",
      "want 0.34549974162753905\n",
      "training loss 0.3553378773580844\n",
      "training accuracy 89.61666666666666\n",
      "epoch 239\n",
      "validation loss 0.3436760361688634\n",
      "want 0.3436760361688632\n",
      "training loss 0.3535961397525597\n",
      "training accuracy 89.7\n",
      "epoch 240\n",
      "validation loss 0.34186189865492767\n",
      "want 0.34186189865492767\n",
      "training loss 0.35186384166791085\n",
      "training accuracy 89.725\n",
      "epoch 241\n",
      "validation loss 0.34005736761659544\n",
      "want 0.34005736761659533\n",
      "training loss 0.350141005160393\n",
      "training accuracy 89.79166666666667\n",
      "epoch 242\n",
      "validation loss 0.338262480664137\n",
      "want 0.3382624806641368\n",
      "training loss 0.34842765147699767\n",
      "training accuracy 89.89166666666667\n",
      "epoch 243\n",
      "validation loss 0.33647727452357296\n",
      "want 0.33647727452357323\n",
      "training loss 0.3467238011052167\n",
      "training accuracy 89.95\n",
      "epoch 244\n",
      "validation loss 0.3347017850671886\n",
      "want 0.3347017850671889\n",
      "training loss 0.34502947381656907\n",
      "training accuracy 90.03333333333333\n",
      "epoch 245\n",
      "validation loss 0.3329360473384427\n",
      "want 0.3329360473384428\n",
      "training loss 0.3433446887040563\n",
      "training accuracy 90.13333333333333\n",
      "epoch 246\n",
      "validation loss 0.33118009557152794\n",
      "want 0.3311800955715278\n",
      "training loss 0.3416694642137931\n",
      "training accuracy 90.16666666666666\n",
      "epoch 247\n",
      "validation loss 0.32943396320582974\n",
      "want 0.3294339632058296\n",
      "training loss 0.34000381817108627\n",
      "training accuracy 90.25\n",
      "epoch 248\n",
      "validation loss 0.32769768289554946\n",
      "want 0.3276976828955493\n",
      "training loss 0.33834776780120907\n",
      "training accuracy 90.33333333333333\n",
      "epoch 249\n",
      "validation loss 0.32597128651475743\n",
      "want 0.3259712865147573\n",
      "training loss 0.33670132974512185\n",
      "training accuracy 90.38333333333334\n",
      "epoch 250\n",
      "validation loss 0.32425480515815447\n",
      "want 0.324254805158155\n",
      "training loss 0.3350645200704482\n",
      "training accuracy 90.43333333333334\n",
      "epoch 251\n",
      "validation loss 0.3225482691378197\n",
      "want 0.3225482691378197\n",
      "training loss 0.3334373542779722\n",
      "training accuracy 90.54166666666667\n",
      "epoch 252\n",
      "validation loss 0.32085170797621637\n",
      "want 0.3208517079762159\n",
      "training loss 0.33181984730391595\n",
      "training accuracy 90.60833333333333\n",
      "epoch 253\n",
      "validation loss 0.31916515039575455\n",
      "want 0.31916515039575494\n",
      "training loss 0.33021201351834006\n",
      "training accuracy 90.68333333333334\n",
      "epoch 254\n",
      "validation loss 0.3174886243051827\n",
      "want 0.3174886243051827\n",
      "training loss 0.32861386671988985\n",
      "training accuracy 90.78333333333333\n",
      "epoch 255\n",
      "validation loss 0.3158221567830825\n",
      "want 0.3158221567830822\n",
      "training loss 0.32702542012722385\n",
      "training accuracy 90.85\n",
      "epoch 256\n",
      "validation loss 0.31416577405876417\n",
      "want 0.3141657740587641\n",
      "training loss 0.32544668636739055\n",
      "training accuracy 90.86666666666666\n",
      "epoch 257\n",
      "validation loss 0.31251950149082797\n",
      "want 0.3125195014908278\n",
      "training loss 0.32387767746143525\n",
      "training accuracy 90.875\n",
      "epoch 258\n",
      "validation loss 0.3108833635436591\n",
      "want 0.3108833635436596\n",
      "training loss 0.3223184048075214\n",
      "training accuracy 90.93333333333334\n",
      "epoch 259\n",
      "validation loss 0.30925738376214174\n",
      "want 0.30925738376214157\n",
      "training loss 0.3207688791618456\n",
      "training accuracy 90.99166666666667\n",
      "epoch 260\n",
      "validation loss 0.30764158474482717\n",
      "want 0.30764158474482733\n",
      "training loss 0.3192291106176113\n",
      "training accuracy 91.025\n",
      "epoch 261\n",
      "validation loss 0.30603598811584454\n",
      "want 0.3060359881158443\n",
      "training loss 0.3176991085823166\n",
      "training accuracy 91.06666666666666\n",
      "epoch 262\n",
      "validation loss 0.3044406144957681\n",
      "want 0.30444061449576826\n",
      "training loss 0.31617888175364345\n",
      "training accuracy 91.10833333333333\n",
      "epoch 263\n",
      "validation loss 0.30285548347171115\n",
      "want 0.3028554834717112\n",
      "training loss 0.3146684380941494\n",
      "training accuracy 91.13333333333333\n",
      "epoch 264\n",
      "validation loss 0.301280613566854\n",
      "want 0.3012806135668543\n",
      "training loss 0.31316778480504465\n",
      "training accuracy 91.175\n",
      "epoch 265\n",
      "validation loss 0.29971602220964993\n",
      "want 0.2997160222096493\n",
      "training loss 0.31167692829928234\n",
      "training accuracy 91.23333333333333\n",
      "epoch 266\n",
      "validation loss 0.29816172570290095\n",
      "want 0.2981617257029009\n",
      "training loss 0.3101958741741297\n",
      "training accuracy 91.27499999999999\n",
      "epoch 267\n",
      "validation loss 0.2966177391929341\n",
      "want 0.29661773919293366\n",
      "training loss 0.308724627183522\n",
      "training accuracy 91.31666666666666\n",
      "epoch 268\n",
      "validation loss 0.2950840766390389\n",
      "want 0.295084076639039\n",
      "training loss 0.30726319121032286\n",
      "training accuracy 91.36666666666666\n",
      "epoch 269\n",
      "validation loss 0.29356075078338195\n",
      "want 0.29356075078338223\n",
      "training loss 0.30581156923872244\n",
      "training accuracy 91.4\n",
      "epoch 270\n",
      "validation loss 0.29204777312154395\n",
      "want 0.29204777312154384\n",
      "training loss 0.30436976332693905\n",
      "training accuracy 91.44166666666666\n",
      "epoch 271\n",
      "validation loss 0.29054515387385615\n",
      "want 0.29054515387385627\n",
      "training loss 0.30293777458039917\n",
      "training accuracy 91.48333333333333\n",
      "epoch 272\n",
      "validation loss 0.2890529019576862\n",
      "want 0.28905290195768674\n",
      "training loss 0.3015156031255525\n",
      "training accuracy 91.55\n",
      "epoch 273\n",
      "validation loss 0.2875710249608048\n",
      "want 0.2875710249608044\n",
      "training loss 0.3001032480844631\n",
      "training accuracy 91.59166666666667\n",
      "epoch 274\n",
      "validation loss 0.28609952911596204\n",
      "want 0.28609952911596204\n",
      "training loss 0.2987007075503246\n",
      "training accuracy 91.63333333333334\n",
      "epoch 275\n",
      "validation loss 0.2846384192768088\n",
      "want 0.2846384192768088\n",
      "training loss 0.2973079785640071\n",
      "training accuracy 91.66666666666666\n",
      "epoch 276\n",
      "validation loss 0.28318769889523904\n",
      "want 0.28318769889523937\n",
      "training loss 0.29592505709176525\n",
      "training accuracy 91.725\n",
      "epoch 277\n",
      "validation loss 0.28174737000027833\n",
      "want 0.2817473700002785\n",
      "training loss 0.29455193800419577\n",
      "training accuracy 91.79166666666667\n",
      "epoch 278\n",
      "validation loss 0.2803174331785814\n",
      "want 0.28031743317858127\n",
      "training loss 0.2931886150565432\n",
      "training accuracy 91.825\n",
      "epoch 279\n",
      "validation loss 0.27889788755662726\n",
      "want 0.27889788755662753\n",
      "training loss 0.29183508087043547\n",
      "training accuracy 91.875\n",
      "epoch 280\n",
      "validation loss 0.2774887307846731\n",
      "want 0.27748873078467295\n",
      "training loss 0.2904913269171126\n",
      "training accuracy 91.98333333333333\n",
      "epoch 281\n",
      "validation loss 0.27608995902251077\n",
      "want 0.2760899590225106\n",
      "training loss 0.28915734350221517\n",
      "training accuracy 92.04166666666667\n",
      "epoch 282\n",
      "validation loss 0.27470156692708886\n",
      "want 0.27470156692708914\n",
      "training loss 0.2878331197521808\n",
      "training accuracy 92.07499999999999\n",
      "epoch 283\n",
      "validation loss 0.2733235476420217\n",
      "want 0.2733235476420215\n",
      "training loss 0.28651864360229584\n",
      "training accuracy 92.10000000000001\n",
      "epoch 284\n",
      "validation loss 0.2719558927890103\n",
      "want 0.27195589278901033\n",
      "training loss 0.28521390178640876\n",
      "training accuracy 92.16666666666666\n",
      "epoch 285\n",
      "validation loss 0.2705985924612089\n",
      "want 0.27059859246120854\n",
      "training loss 0.2839188798283717\n",
      "training accuracy 92.20833333333334\n",
      "epoch 286\n",
      "validation loss 0.2692516352185233\n",
      "want 0.26925163521852336\n",
      "training loss 0.2826335620351803\n",
      "training accuracy 92.23333333333333\n",
      "epoch 287\n",
      "validation loss 0.2679150080848651\n",
      "want 0.2679150080848646\n",
      "training loss 0.2813579314918548\n",
      "training accuracy 92.30000000000001\n",
      "epoch 288\n",
      "validation loss 0.26658869654733314\n",
      "want 0.266588696547333\n",
      "training loss 0.28009197005803665\n",
      "training accuracy 92.30833333333334\n",
      "epoch 289\n",
      "validation loss 0.26527268455733255\n",
      "want 0.26527268455733216\n",
      "training loss 0.278835658366313\n",
      "training accuracy 92.35\n",
      "epoch 290\n",
      "validation loss 0.2639669545335871\n",
      "want 0.263966954533587\n",
      "training loss 0.2775889758222226\n",
      "training accuracy 92.4\n",
      "epoch 291\n",
      "validation loss 0.2626714873670401\n",
      "want 0.26267148736704\n",
      "training loss 0.27635190060597975\n",
      "training accuracy 92.45833333333333\n",
      "epoch 292\n",
      "validation loss 0.2613862624275951\n",
      "want 0.26138626242759505\n",
      "training loss 0.27512440967582374\n",
      "training accuracy 92.51666666666667\n",
      "epoch 293\n",
      "validation loss 0.2601112575726696\n",
      "want 0.26011125757266973\n",
      "training loss 0.27390647877300944\n",
      "training accuracy 92.56666666666666\n",
      "epoch 294\n",
      "validation loss 0.2588464491575147\n",
      "want 0.2588464491575146\n",
      "training loss 0.2726980824283899\n",
      "training accuracy 92.60000000000001\n",
      "epoch 295\n",
      "validation loss 0.25759181204725173\n",
      "want 0.25759181204725207\n",
      "training loss 0.27149919397053773\n",
      "training accuracy 92.60833333333333\n",
      "epoch 296\n",
      "validation loss 0.2563473196305831\n",
      "want 0.2563473196305831\n",
      "training loss 0.2703097855353836\n",
      "training accuracy 92.64166666666667\n",
      "epoch 297\n",
      "validation loss 0.2551129438351082\n",
      "want 0.25511294383510796\n",
      "training loss 0.2691298280772919\n",
      "training accuracy 92.675\n",
      "epoch 298\n",
      "validation loss 0.2538886551442022\n",
      "want 0.25388865514420195\n",
      "training loss 0.2679592913815742\n",
      "training accuracy 92.7\n",
      "epoch 299\n",
      "validation loss 0.25267442261538486\n",
      "want 0.252674422615385\n",
      "training loss 0.26679814407831\n",
      "training accuracy 92.75833333333333\n",
      "epoch 300\n",
      "validation loss 0.25147021390012186\n",
      "want 0.25147021390012186\n",
      "training loss 0.26564635365749883\n",
      "training accuracy 92.81666666666666\n",
      "epoch 301\n",
      "validation loss 0.25027599526498717\n",
      "want 0.2502759952649869\n",
      "training loss 0.26450388648543177\n",
      "training accuracy 92.86666666666666\n",
      "epoch 302\n",
      "validation loss 0.24909173161412543\n",
      "want 0.24909173161412554\n",
      "training loss 0.2633707078222319\n",
      "training accuracy 92.91666666666667\n",
      "epoch 303\n",
      "validation loss 0.24791738651294368\n",
      "want 0.24791738651294362\n",
      "training loss 0.2622467818405192\n",
      "training accuracy 92.95\n",
      "epoch 304\n",
      "validation loss 0.24675292221295447\n",
      "want 0.2467529222129545\n",
      "training loss 0.26113207164512403\n",
      "training accuracy 92.96666666666667\n",
      "epoch 305\n",
      "validation loss 0.24559829967771243\n",
      "want 0.2455982996777126\n",
      "training loss 0.2600265392937842\n",
      "training accuracy 93.025\n",
      "epoch 306\n",
      "validation loss 0.2444534786097619\n",
      "want 0.24445347860976172\n",
      "training loss 0.258930145818753\n",
      "training accuracy 93.05833333333334\n",
      "epoch 307\n",
      "validation loss 0.24331841747852698\n",
      "want 0.24331841747852712\n",
      "training loss 0.2578428512492964\n",
      "training accuracy 93.08333333333333\n",
      "epoch 308\n",
      "validation loss 0.2421930735490788\n",
      "want 0.2421930735490788\n",
      "training loss 0.256764614634946\n",
      "training accuracy 93.10000000000001\n",
      "epoch 309\n",
      "validation loss 0.24107740291169458\n",
      "want 0.24107740291169477\n",
      "training loss 0.2556953940695135\n",
      "training accuracy 93.10833333333333\n",
      "epoch 310\n",
      "validation loss 0.23997136051215393\n",
      "want 0.23997136051215381\n",
      "training loss 0.25463514671575377\n",
      "training accuracy 93.125\n",
      "epoch 311\n",
      "validation loss 0.2388749001826871\n",
      "want 0.23887490018268712\n",
      "training loss 0.25358382883063957\n",
      "training accuracy 93.16666666666666\n",
      "epoch 312\n",
      "validation loss 0.23778797467351967\n",
      "want 0.23778797467351973\n",
      "training loss 0.2525413957911754\n",
      "training accuracy 93.20833333333334\n",
      "epoch 313\n",
      "validation loss 0.23671053568493378\n",
      "want 0.2367105356849338\n",
      "training loss 0.25150780212069324\n",
      "training accuracy 93.23333333333333\n",
      "epoch 314\n",
      "validation loss 0.23564253389978684\n",
      "want 0.23564253389978682\n",
      "training loss 0.25048300151556885\n",
      "training accuracy 93.25833333333333\n",
      "epoch 315\n",
      "validation loss 0.23458391901641962\n",
      "want 0.23458391901641962\n",
      "training loss 0.24946694687231102\n",
      "training accuracy 93.28333333333333\n",
      "epoch 316\n",
      "validation loss 0.23353463978188999\n",
      "want 0.23353463978188993\n",
      "training loss 0.24845959031494597\n",
      "training accuracy 93.30833333333334\n",
      "epoch 317\n",
      "validation loss 0.2324946440254705\n",
      "want 0.23249464402547046\n",
      "training loss 0.24746088322265905\n",
      "training accuracy 93.31666666666666\n",
      "epoch 318\n",
      "validation loss 0.23146387869235008\n",
      "want 0.23146387869235024\n",
      "training loss 0.24647077625764047\n",
      "training accuracy 93.34166666666667\n",
      "epoch 319\n",
      "validation loss 0.23044228987748214\n",
      "want 0.23044228987748186\n",
      "training loss 0.24548921939307036\n",
      "training accuracy 93.375\n",
      "epoch 320\n",
      "validation loss 0.22942982285951785\n",
      "want 0.2294298228595179\n",
      "training loss 0.24451616194120868\n",
      "training accuracy 93.4\n",
      "epoch 321\n",
      "validation loss 0.2284264221347822\n",
      "want 0.22842642213478234\n",
      "training loss 0.24355155258153288\n",
      "training accuracy 93.43333333333334\n",
      "epoch 322\n",
      "validation loss 0.22743203145122504\n",
      "want 0.22743203145122515\n",
      "training loss 0.24259533938887493\n",
      "training accuracy 93.46666666666667\n",
      "epoch 323\n",
      "validation loss 0.22644659384231003\n",
      "want 0.22644659384231033\n",
      "training loss 0.24164746986152894\n",
      "training accuracy 93.475\n",
      "epoch 324\n",
      "validation loss 0.22547005166078926\n",
      "want 0.22547005166078932\n",
      "training loss 0.24070789094927666\n",
      "training accuracy 93.525\n",
      "epoch 325\n",
      "validation loss 0.22450234661231439\n",
      "want 0.22450234661231463\n",
      "training loss 0.23977654908127416\n",
      "training accuracy 93.55833333333334\n",
      "epoch 326\n",
      "validation loss 0.2235434197888504\n",
      "want 0.2235434197888506\n",
      "training loss 0.23885339019380944\n",
      "training accuracy 93.58333333333333\n",
      "epoch 327\n",
      "validation loss 0.22259321170184032\n",
      "want 0.22259321170184035\n",
      "training loss 0.23793835975783875\n",
      "training accuracy 93.625\n",
      "epoch 328\n",
      "validation loss 0.22165166231509018\n",
      "want 0.22165166231508998\n",
      "training loss 0.23703140280631588\n",
      "training accuracy 93.675\n",
      "epoch 329\n",
      "validation loss 0.2207187110773338\n",
      "want 0.22071871107733432\n",
      "training loss 0.23613246396124915\n",
      "training accuracy 93.71666666666667\n",
      "epoch 330\n",
      "validation loss 0.2197942969544493\n",
      "want 0.21979429695444908\n",
      "training loss 0.2352414874604709\n",
      "training accuracy 93.75833333333333\n",
      "epoch 331\n",
      "validation loss 0.21887835846127837\n",
      "want 0.21887835846127832\n",
      "training loss 0.23435841718408826\n",
      "training accuracy 93.77499999999999\n",
      "epoch 332\n",
      "validation loss 0.21797083369304682\n",
      "want 0.2179708336930469\n",
      "training loss 0.23348319668061124\n",
      "training accuracy 93.8\n",
      "epoch 333\n",
      "validation loss 0.21707166035633058\n",
      "want 0.21707166035633038\n",
      "training loss 0.23261576919268573\n",
      "training accuracy 93.85\n",
      "epoch 334\n",
      "validation loss 0.21618077579955663\n",
      "want 0.21618077579955666\n",
      "training loss 0.23175607768244924\n",
      "training accuracy 93.89999999999999\n",
      "epoch 335\n",
      "validation loss 0.21529811704301632\n",
      "want 0.21529811704301613\n",
      "training loss 0.23090406485648501\n",
      "training accuracy 93.93333333333334\n",
      "epoch 336\n",
      "validation loss 0.21442362080835808\n",
      "want 0.21442362080835825\n",
      "training loss 0.23005967319034998\n",
      "training accuracy 93.95833333333333\n",
      "epoch 337\n",
      "validation loss 0.2135572235475553\n",
      "want 0.21355722354755527\n",
      "training loss 0.22922284495264075\n",
      "training accuracy 93.975\n",
      "epoch 338\n",
      "validation loss 0.21269886147131598\n",
      "want 0.2126988614713159\n",
      "training loss 0.22839352222861883\n",
      "training accuracy 93.98333333333333\n",
      "epoch 339\n",
      "validation loss 0.21184847057693135\n",
      "want 0.2118484705769312\n",
      "training loss 0.2275716469433577\n",
      "training accuracy 94.03333333333333\n",
      "epoch 340\n",
      "validation loss 0.21100598667554143\n",
      "want 0.21100598667554119\n",
      "training loss 0.22675716088440565\n",
      "training accuracy 94.04166666666667\n",
      "epoch 341\n",
      "validation loss 0.21017134541880692\n",
      "want 0.2101713454188068\n",
      "training loss 0.22595000572394902\n",
      "training accuracy 94.05833333333334\n",
      "epoch 342\n",
      "validation loss 0.20934448232497954\n",
      "want 0.20934448232497935\n",
      "training loss 0.22515012304048448\n",
      "training accuracy 94.06666666666666\n",
      "epoch 343\n",
      "validation loss 0.20852533280435537\n",
      "want 0.2085253328043556\n",
      "training loss 0.22435745433996168\n",
      "training accuracy 94.08333333333333\n",
      "epoch 344\n",
      "validation loss 0.20771383218411235\n",
      "want 0.20771383218411243\n",
      "training loss 0.2235719410764246\n",
      "training accuracy 94.1\n",
      "epoch 345\n",
      "validation loss 0.2069099157325141\n",
      "want 0.20690991573251422\n",
      "training loss 0.22279352467211677\n",
      "training accuracy 94.13333333333334\n",
      "epoch 346\n",
      "validation loss 0.20611351868248814\n",
      "want 0.20611351868248776\n",
      "training loss 0.22202214653706803\n",
      "training accuracy 94.125\n",
      "epoch 347\n",
      "validation loss 0.20532457625456163\n",
      "want 0.2053245762545618\n",
      "training loss 0.22125774808815016\n",
      "training accuracy 94.15833333333333\n",
      "epoch 348\n",
      "validation loss 0.20454302367916824\n",
      "want 0.20454302367916813\n",
      "training loss 0.22050027076759574\n",
      "training accuracy 94.175\n",
      "epoch 349\n",
      "validation loss 0.20376879621830304\n",
      "want 0.20376879621830327\n",
      "training loss 0.21974965606098001\n",
      "training accuracy 94.16666666666667\n",
      "epoch 350\n",
      "validation loss 0.20300182918655058\n",
      "want 0.20300182918655107\n",
      "training loss 0.21900584551469357\n",
      "training accuracy 94.175\n",
      "epoch 351\n",
      "validation loss 0.2022420579714663\n",
      "want 0.2022420579714665\n",
      "training loss 0.21826878075284917\n",
      "training accuracy 94.19999999999999\n",
      "epoch 352\n",
      "validation loss 0.20148941805332293\n",
      "want 0.20148941805332313\n",
      "training loss 0.2175384034936878\n",
      "training accuracy 94.22500000000001\n",
      "epoch 353\n",
      "validation loss 0.2007438450242266\n",
      "want 0.20074384502422685\n",
      "training loss 0.21681465556543938\n",
      "training accuracy 94.24166666666667\n",
      "epoch 354\n",
      "validation loss 0.20000527460659995\n",
      "want 0.20000527460660014\n",
      "training loss 0.2160974789216714\n",
      "training accuracy 94.25\n",
      "epoch 355\n",
      "validation loss 0.19927364267104072\n",
      "want 0.19927364267104056\n",
      "training loss 0.21538681565610693\n",
      "training accuracy 94.27499999999999\n",
      "epoch 356\n",
      "validation loss 0.19854888525355974\n",
      "want 0.19854888525355954\n",
      "training loss 0.21468260801693737\n",
      "training accuracy 94.3\n",
      "epoch 357\n",
      "validation loss 0.1978309385722076\n",
      "want 0.19783093857220746\n",
      "training loss 0.2139847984206198\n",
      "training accuracy 94.30833333333334\n",
      "epoch 358\n",
      "validation loss 0.1971197390430911\n",
      "want 0.19711973904309107\n",
      "training loss 0.213293329465164\n",
      "training accuracy 94.31666666666668\n",
      "epoch 359\n",
      "validation loss 0.19641522329579153\n",
      "want 0.19641522329579142\n",
      "training loss 0.21260814394294106\n",
      "training accuracy 94.31666666666668\n",
      "epoch 360\n",
      "validation loss 0.19571732818818935\n",
      "want 0.195717328188189\n",
      "training loss 0.21192918485297646\n",
      "training accuracy 94.325\n",
      "epoch 361\n",
      "validation loss 0.19502599082070612\n",
      "want 0.19502599082070587\n",
      "training loss 0.21125639541277105\n",
      "training accuracy 94.35\n",
      "epoch 362\n",
      "validation loss 0.1943411485499721\n",
      "want 0.19434114854997217\n",
      "training loss 0.21058971906965307\n",
      "training accuracy 94.35\n",
      "epoch 363\n",
      "validation loss 0.1936627390019267\n",
      "want 0.1936627390019266\n",
      "training loss 0.20992909951164346\n",
      "training accuracy 94.35\n",
      "epoch 364\n",
      "validation loss 0.1929907000843617\n",
      "want 0.1929907000843615\n",
      "training loss 0.2092744806778767\n",
      "training accuracy 94.38333333333333\n",
      "epoch 365\n",
      "validation loss 0.19232496999892013\n",
      "want 0.19232496999892015\n",
      "training loss 0.20862580676855122\n",
      "training accuracy 94.39999999999999\n",
      "epoch 366\n",
      "validation loss 0.19166548725255902\n",
      "want 0.19166548725255905\n",
      "training loss 0.20798302225445484\n",
      "training accuracy 94.40833333333333\n",
      "epoch 367\n",
      "validation loss 0.19101219066848363\n",
      "want 0.19101219066848357\n",
      "training loss 0.207346071886042\n",
      "training accuracy 94.425\n",
      "epoch 368\n",
      "validation loss 0.1903650193965686\n",
      "want 0.19036501939656875\n",
      "training loss 0.20671490070209442\n",
      "training accuracy 94.44166666666666\n",
      "epoch 369\n",
      "validation loss 0.1897239129232756\n",
      "want 0.1897239129232754\n",
      "training loss 0.20608945403794973\n",
      "training accuracy 94.44166666666666\n",
      "epoch 370\n",
      "validation loss 0.1890888110810734\n",
      "want 0.18908881108107328\n",
      "training loss 0.20546967753333648\n",
      "training accuracy 94.46666666666667\n",
      "epoch 371\n",
      "validation loss 0.18845965405738133\n",
      "want 0.18845965405738152\n",
      "training loss 0.2048555171398109\n",
      "training accuracy 94.45\n",
      "epoch 372\n",
      "validation loss 0.18783638240303832\n",
      "want 0.18783638240303818\n",
      "training loss 0.2042469191277868\n",
      "training accuracy 94.45\n",
      "epoch 373\n",
      "validation loss 0.18721893704030998\n",
      "want 0.18721893704031004\n",
      "training loss 0.20364383009320317\n",
      "training accuracy 94.45\n",
      "epoch 374\n",
      "validation loss 0.186607259270454\n",
      "want 0.18660725927045382\n",
      "training loss 0.20304619696381696\n",
      "training accuracy 94.46666666666667\n",
      "epoch 375\n",
      "validation loss 0.18600129078084068\n",
      "want 0.1860012907808404\n",
      "training loss 0.20245396700513693\n",
      "training accuracy 94.49166666666666\n",
      "epoch 376\n",
      "validation loss 0.1854009736516532\n",
      "want 0.18540097365165303\n",
      "training loss 0.20186708782598292\n",
      "training accuracy 94.5\n",
      "epoch 377\n",
      "validation loss 0.18480625036217155\n",
      "want 0.18480625036217124\n",
      "training loss 0.20128550738375064\n",
      "training accuracy 94.51666666666667\n",
      "epoch 378\n",
      "validation loss 0.18421706379665206\n",
      "want 0.18421706379665195\n",
      "training loss 0.20070917398929036\n",
      "training accuracy 94.53333333333333\n",
      "epoch 379\n",
      "validation loss 0.18363335724981775\n",
      "want 0.18363335724981789\n",
      "training loss 0.20013803631150248\n",
      "training accuracy 94.55\n",
      "epoch 380\n",
      "validation loss 0.18305507443196622\n",
      "want 0.1830550744319662\n",
      "training loss 0.19957204338159307\n",
      "training accuracy 94.56666666666666\n",
      "epoch 381\n",
      "validation loss 0.18248215947370675\n",
      "want 0.18248215947370683\n",
      "training loss 0.19901114459703748\n",
      "training accuracy 94.61666666666667\n",
      "epoch 382\n",
      "validation loss 0.18191455693034236\n",
      "want 0.18191455693034253\n",
      "training loss 0.19845528972524645\n",
      "training accuracy 94.625\n",
      "epoch 383\n",
      "validation loss 0.18135221178590144\n",
      "want 0.18135221178590114\n",
      "training loss 0.19790442890694449\n",
      "training accuracy 94.65\n",
      "epoch 384\n",
      "validation loss 0.18079506945683121\n",
      "want 0.18079506945683085\n",
      "training loss 0.19735851265926138\n",
      "training accuracy 94.675\n",
      "epoch 385\n",
      "validation loss 0.18024307579536922\n",
      "want 0.18024307579536905\n",
      "training loss 0.19681749187856826\n",
      "training accuracy 94.70833333333334\n",
      "epoch 386\n",
      "validation loss 0.1796961770925956\n",
      "want 0.17969617709259564\n",
      "training loss 0.1962813178430454\n",
      "training accuracy 94.71666666666667\n",
      "epoch 387\n",
      "validation loss 0.17915432008118035\n",
      "want 0.17915432008118043\n",
      "training loss 0.19574994221499536\n",
      "training accuracy 94.74166666666667\n",
      "epoch 388\n",
      "validation loss 0.17861745193783535\n",
      "want 0.1786174519378356\n",
      "training loss 0.195223317042921\n",
      "training accuracy 94.76666666666667\n",
      "epoch 389\n",
      "validation loss 0.1780855202854826\n",
      "want 0.17808552028548258\n",
      "training loss 0.19470139476335616\n",
      "training accuracy 94.76666666666667\n",
      "epoch 390\n",
      "validation loss 0.17755847319514353\n",
      "want 0.17755847319514373\n",
      "training loss 0.19418412820247943\n",
      "training accuracy 94.8\n",
      "epoch 391\n",
      "validation loss 0.17703625918756788\n",
      "want 0.17703625918756788\n",
      "training loss 0.19367147057750045\n",
      "training accuracy 94.81666666666668\n",
      "epoch 392\n",
      "validation loss 0.17651882723460016\n",
      "want 0.17651882723460013\n",
      "training loss 0.19316337549783985\n",
      "training accuracy 94.80833333333332\n",
      "epoch 393\n",
      "validation loss 0.17600612676030405\n",
      "want 0.17600612676030414\n",
      "training loss 0.1926597969661034\n",
      "training accuracy 94.80833333333332\n",
      "epoch 394\n",
      "validation loss 0.17549810764184684\n",
      "want 0.17549810764184684\n",
      "training loss 0.19216068937885328\n",
      "training accuracy 94.81666666666668\n",
      "epoch 395\n",
      "validation loss 0.1749947202101542\n",
      "want 0.17499472021015416\n",
      "training loss 0.19166600752719753\n",
      "training accuracy 94.81666666666668\n",
      "epoch 396\n",
      "validation loss 0.17449591525034652\n",
      "want 0.17449591525034652\n",
      "training loss 0.1911757065972054\n",
      "training accuracy 94.84166666666667\n",
      "epoch 397\n",
      "validation loss 0.1740016440019627\n",
      "want 0.17400164400196275\n",
      "training loss 0.1906897421701141\n",
      "training accuracy 94.86666666666666\n",
      "epoch 398\n",
      "validation loss 0.17351185815898063\n",
      "want 0.17351185815898063\n",
      "training loss 0.19020807022241193\n",
      "training accuracy 94.89166666666667\n",
      "epoch 399\n",
      "validation loss 0.17302650986964238\n",
      "want 0.17302650986964233\n",
      "training loss 0.18973064712572393\n",
      "training accuracy 94.89999999999999\n",
      "epoch 400\n",
      "validation loss 0.17254555173609254\n",
      "want 0.17254555173609262\n",
      "training loss 0.1892574296465591\n",
      "training accuracy 94.90833333333333\n",
      "epoch 401\n",
      "validation loss 0.17206893681383736\n",
      "want 0.17206893681383748\n",
      "training loss 0.18878837494590822\n",
      "training accuracy 94.90833333333333\n",
      "epoch 402\n",
      "validation loss 0.17159661861103123\n",
      "want 0.17159661861103115\n",
      "training loss 0.18832344057868927\n",
      "training accuracy 94.91666666666667\n",
      "epoch 403\n",
      "validation loss 0.17112855108759817\n",
      "want 0.17112855108759842\n",
      "training loss 0.18786258449306797\n",
      "training accuracy 94.91666666666667\n",
      "epoch 404\n",
      "validation loss 0.17066468865419998\n",
      "want 0.17066468865419968\n",
      "training loss 0.18740576502963727\n",
      "training accuracy 94.94166666666666\n",
      "epoch 405\n",
      "validation loss 0.17020498617104524\n",
      "want 0.17020498617104535\n",
      "training loss 0.18695294092047868\n",
      "training accuracy 94.94166666666666\n",
      "epoch 406\n",
      "validation loss 0.16974939894656732\n",
      "want 0.1697493989465673\n",
      "training loss 0.18650407128810065\n",
      "training accuracy 94.95\n",
      "epoch 407\n",
      "validation loss 0.16929788273595253\n",
      "want 0.16929788273595256\n",
      "training loss 0.18605911564426067\n",
      "training accuracy 94.95833333333333\n",
      "epoch 408\n",
      "validation loss 0.1688503937395467\n",
      "want 0.16885039373954683\n",
      "training loss 0.18561803388868517\n",
      "training accuracy 94.99166666666666\n",
      "epoch 409\n",
      "validation loss 0.1684068886011339\n",
      "want 0.16840688860113398\n",
      "training loss 0.18518078630767668\n",
      "training accuracy 94.99166666666666\n",
      "epoch 410\n",
      "validation loss 0.16796732440609646\n",
      "want 0.1679673244060964\n",
      "training loss 0.1847473335726269\n",
      "training accuracy 94.99166666666666\n",
      "epoch 411\n",
      "validation loss 0.1675316586794636\n",
      "want 0.16753165867946374\n",
      "training loss 0.18431763673843446\n",
      "training accuracy 95.01666666666667\n",
      "epoch 412\n",
      "validation loss 0.16709984938385455\n",
      "want 0.1670998493838543\n",
      "training loss 0.18389165724183507\n",
      "training accuracy 95.01666666666667\n",
      "epoch 413\n",
      "validation loss 0.16667185491731576\n",
      "want 0.16667185491731554\n",
      "training loss 0.18346935689964153\n",
      "training accuracy 95.01666666666667\n",
      "epoch 414\n",
      "validation loss 0.1662476341110685\n",
      "want 0.16624763411106835\n",
      "training loss 0.1830506979069076\n",
      "training accuracy 95.025\n",
      "epoch 415\n",
      "validation loss 0.16582714622716094\n",
      "want 0.165827146227161\n",
      "training loss 0.1826356428350103\n",
      "training accuracy 95.04166666666667\n",
      "epoch 416\n",
      "validation loss 0.16541035095603693\n",
      "want 0.16541035095603687\n",
      "training loss 0.1822241546296694\n",
      "training accuracy 95.05833333333334\n",
      "epoch 417\n",
      "validation loss 0.1649972084140218\n",
      "want 0.1649972084140216\n",
      "training loss 0.18181619660888373\n",
      "training accuracy 95.06666666666666\n",
      "epoch 418\n",
      "validation loss 0.16458767914073377\n",
      "want 0.16458767914073386\n",
      "training loss 0.18141173246081765\n",
      "training accuracy 95.06666666666666\n",
      "epoch 419\n",
      "validation loss 0.1641817240964245\n",
      "want 0.16418172409642454\n",
      "training loss 0.18101072624161155\n",
      "training accuracy 95.06666666666666\n",
      "epoch 420\n",
      "validation loss 0.16377930465924845\n",
      "want 0.16377930465924853\n",
      "training loss 0.18061314237315476\n",
      "training accuracy 95.05833333333334\n",
      "epoch 421\n",
      "validation loss 0.16338038262247326\n",
      "want 0.16338038262247334\n",
      "training loss 0.18021894564077787\n",
      "training accuracy 95.075\n",
      "epoch 422\n",
      "validation loss 0.16298492019162883\n",
      "want 0.16298492019162886\n",
      "training loss 0.17982810119092296\n",
      "training accuracy 95.08333333333333\n",
      "epoch 423\n",
      "validation loss 0.16259287998160182\n",
      "want 0.16259287998160182\n",
      "training loss 0.17944057452874135\n",
      "training accuracy 95.09166666666667\n",
      "epoch 424\n",
      "validation loss 0.1622042250136796\n",
      "want 0.16220422501367915\n",
      "training loss 0.1790563315156631\n",
      "training accuracy 95.1\n",
      "epoch 425\n",
      "validation loss 0.16181891871254336\n",
      "want 0.16181891871254356\n",
      "training loss 0.17867533836692218\n",
      "training accuracy 95.09166666666667\n",
      "epoch 426\n",
      "validation loss 0.16143692490322484\n",
      "want 0.16143692490322503\n",
      "training loss 0.17829756164903823\n",
      "training accuracy 95.09166666666667\n",
      "epoch 427\n",
      "validation loss 0.1610582078080114\n",
      "want 0.16105820780801144\n",
      "training loss 0.17792296827726314\n",
      "training accuracy 95.09166666666667\n",
      "epoch 428\n",
      "validation loss 0.16068273204332234\n",
      "want 0.16068273204332228\n",
      "training loss 0.17755152551300318\n",
      "training accuracy 95.08333333333333\n",
      "epoch 429\n",
      "validation loss 0.16031046261654747\n",
      "want 0.1603104626165472\n",
      "training loss 0.17718320096119722\n",
      "training accuracy 95.075\n",
      "epoch 430\n",
      "validation loss 0.15994136492285405\n",
      "want 0.15994136492285402\n",
      "training loss 0.17681796256768442\n",
      "training accuracy 95.08333333333333\n",
      "epoch 431\n",
      "validation loss 0.159575404741968\n",
      "want 0.15957540474196802\n",
      "training loss 0.17645577861652217\n",
      "training accuracy 95.08333333333333\n",
      "epoch 432\n",
      "validation loss 0.1592125482349257\n",
      "want 0.15921254823492567\n",
      "training loss 0.17609661772730983\n",
      "training accuracy 95.10833333333333\n",
      "epoch 433\n",
      "validation loss 0.15885276194080547\n",
      "want 0.15885276194080558\n",
      "training loss 0.1757404488524665\n",
      "training accuracy 95.1\n",
      "epoch 434\n",
      "validation loss 0.158496012773439\n",
      "want 0.1584960127734392\n",
      "training loss 0.1753872412745041\n",
      "training accuracy 95.1\n",
      "epoch 435\n",
      "validation loss 0.15814226801810377\n",
      "want 0.15814226801810324\n",
      "training loss 0.17503696460328197\n",
      "training accuracy 95.1\n",
      "epoch 436\n",
      "validation loss 0.15779149532819736\n",
      "want 0.15779149532819747\n",
      "training loss 0.1746895887732355\n",
      "training accuracy 95.10833333333333\n",
      "epoch 437\n",
      "validation loss 0.1574436627219085\n",
      "want 0.15744366272190852\n",
      "training loss 0.17434508404061413\n",
      "training accuracy 95.10833333333333\n",
      "epoch 438\n",
      "validation loss 0.1570987385788629\n",
      "want 0.1570987385788631\n",
      "training loss 0.17400342098068117\n",
      "training accuracy 95.11666666666667\n",
      "epoch 439\n",
      "validation loss 0.1567566916367721\n",
      "want 0.15675669163677233\n",
      "training loss 0.17366457048492706\n",
      "training accuracy 95.10833333333333\n",
      "epoch 440\n",
      "validation loss 0.1564174909880694\n",
      "want 0.15641749098806912\n",
      "training loss 0.17332850375826\n",
      "training accuracy 95.125\n",
      "epoch 441\n",
      "validation loss 0.15608110607654055\n",
      "want 0.15608110607654052\n",
      "training loss 0.1729951923162006\n",
      "training accuracy 95.13333333333334\n",
      "epoch 442\n",
      "validation loss 0.1557475066939569\n",
      "want 0.1557475066939569\n",
      "training loss 0.17266460798206487\n",
      "training accuracy 95.14166666666667\n",
      "epoch 443\n",
      "validation loss 0.15541666297669973\n",
      "want 0.15541666297669984\n",
      "training loss 0.1723367228841452\n",
      "training accuracy 95.15\n",
      "epoch 444\n",
      "validation loss 0.15508854540239067\n",
      "want 0.15508854540239042\n",
      "training loss 0.1720115094528937\n",
      "training accuracy 95.15\n",
      "epoch 445\n",
      "validation loss 0.15476312478651896\n",
      "want 0.15476312478651896\n",
      "training loss 0.1716889404181023\n",
      "training accuracy 95.16666666666667\n",
      "epoch 446\n",
      "validation loss 0.1544403722790784\n",
      "want 0.15444037227907859\n",
      "training loss 0.17136898880608278\n",
      "training accuracy 95.175\n",
      "epoch 447\n",
      "validation loss 0.15412025936120335\n",
      "want 0.15412025936120338\n",
      "training loss 0.17105162793684728\n",
      "training accuracy 95.16666666666667\n",
      "epoch 448\n",
      "validation loss 0.15380275784181313\n",
      "want 0.15380275784181302\n",
      "training loss 0.17073683142129978\n",
      "training accuracy 95.16666666666667\n",
      "epoch 449\n",
      "validation loss 0.15348783985426484\n",
      "want 0.15348783985426467\n",
      "training loss 0.170424573158425\n",
      "training accuracy 95.18333333333334\n",
      "epoch 450\n",
      "validation loss 0.153175477853014\n",
      "want 0.15317547785301383\n",
      "training loss 0.17011482733248276\n",
      "training accuracy 95.175\n",
      "epoch 451\n",
      "validation loss 0.1528656446102853\n",
      "want 0.1528656446102853\n",
      "training loss 0.1698075684102135\n",
      "training accuracy 95.18333333333334\n",
      "epoch 452\n",
      "validation loss 0.1525583132127552\n",
      "want 0.152558313212755\n",
      "training loss 0.1695027711380461\n",
      "training accuracy 95.19999999999999\n",
      "epoch 453\n",
      "validation loss 0.15225345705824445\n",
      "want 0.15225345705824417\n",
      "training loss 0.1692004105393177\n",
      "training accuracy 95.19999999999999\n",
      "epoch 454\n",
      "validation loss 0.15195104985242666\n",
      "want 0.15195104985242688\n",
      "training loss 0.1689004619114993\n",
      "training accuracy 95.19999999999999\n",
      "epoch 455\n",
      "validation loss 0.1516510656055513\n",
      "want 0.1516510656055513\n",
      "training loss 0.16860290082343846\n",
      "training accuracy 95.19999999999999\n",
      "epoch 456\n",
      "validation loss 0.1513534786291767\n",
      "want 0.15135347862917678\n",
      "training loss 0.16830770311260193\n",
      "training accuracy 95.20833333333333\n",
      "epoch 457\n",
      "validation loss 0.15105826353292626\n",
      "want 0.151058263532926\n",
      "training loss 0.1680148448823379\n",
      "training accuracy 95.20833333333333\n",
      "epoch 458\n",
      "validation loss 0.15076539522125468\n",
      "want 0.15076539522125473\n",
      "training loss 0.16772430249915016\n",
      "training accuracy 95.22500000000001\n",
      "epoch 459\n",
      "validation loss 0.15047484889023868\n",
      "want 0.15047484889023857\n",
      "training loss 0.16743605258998043\n",
      "training accuracy 95.23333333333333\n",
      "epoch 460\n",
      "validation loss 0.15018660002437817\n",
      "want 0.15018660002437806\n",
      "training loss 0.16715007203950905\n",
      "training accuracy 95.24166666666667\n",
      "epoch 461\n",
      "validation loss 0.14990062439342314\n",
      "want 0.1499006243934231\n",
      "training loss 0.1668663379874668\n",
      "training accuracy 95.28333333333333\n",
      "epoch 462\n",
      "validation loss 0.1496168980492162\n",
      "want 0.1496168980492163\n",
      "training loss 0.16658482782596207\n",
      "training accuracy 95.29166666666666\n",
      "epoch 463\n",
      "validation loss 0.149335397322557\n",
      "want 0.14933539732255718\n",
      "training loss 0.16630551919682626\n",
      "training accuracy 95.29166666666666\n",
      "epoch 464\n",
      "validation loss 0.1490560988200868\n",
      "want 0.14905609882008655\n",
      "training loss 0.16602838998897176\n",
      "training accuracy 95.29166666666666\n",
      "epoch 465\n",
      "validation loss 0.14877897942119253\n",
      "want 0.14877897942119261\n",
      "training loss 0.1657534183357619\n",
      "training accuracy 95.3\n",
      "epoch 466\n",
      "validation loss 0.1485040162749383\n",
      "want 0.14850401627493834\n",
      "training loss 0.16548058261240872\n",
      "training accuracy 95.3\n",
      "epoch 467\n",
      "validation loss 0.14823118679701172\n",
      "want 0.14823118679701167\n",
      "training loss 0.16520986143338223\n",
      "training accuracy 95.30833333333332\n",
      "epoch 468\n",
      "validation loss 0.14796046866669788\n",
      "want 0.14796046866669768\n",
      "training loss 0.1649412336498273\n",
      "training accuracy 95.30833333333332\n",
      "epoch 469\n",
      "validation loss 0.1476918398238744\n",
      "want 0.14769183982387424\n",
      "training loss 0.16467467834701804\n",
      "training accuracy 95.30833333333332\n",
      "epoch 470\n",
      "validation loss 0.1474252784660309\n",
      "want 0.14742527846603087\n",
      "training loss 0.16441017484181386\n",
      "training accuracy 95.3\n",
      "epoch 471\n",
      "validation loss 0.14716076304531125\n",
      "want 0.14716076304531134\n",
      "training loss 0.16414770268013992\n",
      "training accuracy 95.3\n",
      "epoch 472\n",
      "validation loss 0.1468982722655802\n",
      "want 0.1468982722655801\n",
      "training loss 0.16388724163449023\n",
      "training accuracy 95.3\n",
      "epoch 473\n",
      "validation loss 0.14663778507951347\n",
      "want 0.14663778507951356\n",
      "training loss 0.16362877170144036\n",
      "training accuracy 95.29166666666666\n",
      "epoch 474\n",
      "validation loss 0.14637928068571504\n",
      "want 0.14637928068571504\n",
      "training loss 0.1633722730991879\n",
      "training accuracy 95.30833333333332\n",
      "epoch 475\n",
      "validation loss 0.14612273852585525\n",
      "want 0.14612273852585533\n",
      "training loss 0.16311772626511054\n",
      "training accuracy 95.30833333333332\n",
      "epoch 476\n",
      "validation loss 0.14586813828183787\n",
      "want 0.14586813828183784\n",
      "training loss 0.1628651118533406\n",
      "training accuracy 95.30833333333332\n",
      "epoch 477\n",
      "validation loss 0.14561545987298857\n",
      "want 0.1456154598729886\n",
      "training loss 0.162614410732359\n",
      "training accuracy 95.33333333333334\n",
      "epoch 478\n",
      "validation loss 0.1453646834532727\n",
      "want 0.14536468345327244\n",
      "training loss 0.16236560398261538\n",
      "training accuracy 95.34166666666667\n",
      "epoch 479\n",
      "validation loss 0.1451157894085337\n",
      "want 0.14511578940853362\n",
      "training loss 0.16211867289416357\n",
      "training accuracy 95.34166666666667\n",
      "epoch 480\n",
      "validation loss 0.14486875835376276\n",
      "want 0.14486875835376276\n",
      "training loss 0.16187359896432\n",
      "training accuracy 95.35833333333333\n",
      "epoch 481\n",
      "validation loss 0.1446235711303891\n",
      "want 0.14462357113038912\n",
      "training loss 0.1616303638953332\n",
      "training accuracy 95.36666666666666\n",
      "epoch 482\n",
      "validation loss 0.14438020880359853\n",
      "want 0.14438020880359864\n",
      "training loss 0.16138894959209457\n",
      "training accuracy 95.36666666666666\n",
      "epoch 483\n",
      "validation loss 0.14413865265967823\n",
      "want 0.14413865265967812\n",
      "training loss 0.16114933815984303\n",
      "training accuracy 95.375\n",
      "epoch 484\n",
      "validation loss 0.14389888420338465\n",
      "want 0.14389888420338462\n",
      "training loss 0.16091151190191427\n",
      "training accuracy 95.36666666666666\n",
      "epoch 485\n",
      "validation loss 0.14366088515534167\n",
      "want 0.1436608851553416\n",
      "training loss 0.16067545331749467\n",
      "training accuracy 95.36666666666666\n",
      "epoch 486\n",
      "validation loss 0.14342463744946027\n",
      "want 0.14342463744946024\n",
      "training loss 0.16044114509940358\n",
      "training accuracy 95.36666666666666\n",
      "epoch 487\n",
      "validation loss 0.1431901232303873\n",
      "want 0.14319012323038727\n",
      "training loss 0.1602085701318953\n",
      "training accuracy 95.375\n",
      "epoch 488\n",
      "validation loss 0.1429573248509783\n",
      "want 0.14295732485097815\n",
      "training loss 0.15997771148847975\n",
      "training accuracy 95.36666666666666\n",
      "epoch 489\n",
      "validation loss 0.14272622486979697\n",
      "want 0.14272622486979686\n",
      "training loss 0.15974855242976746\n",
      "training accuracy 95.375\n",
      "epoch 490\n",
      "validation loss 0.14249680604864118\n",
      "want 0.1424968060486409\n",
      "training loss 0.15952107640133195\n",
      "training accuracy 95.39166666666667\n",
      "epoch 491\n",
      "validation loss 0.1422690513500924\n",
      "want 0.1422690513500924\n",
      "training loss 0.15929526703159216\n",
      "training accuracy 95.38333333333333\n",
      "epoch 492\n",
      "validation loss 0.14204294393509523\n",
      "want 0.1420429439350952\n",
      "training loss 0.15907110812972455\n",
      "training accuracy 95.39166666666667\n",
      "epoch 493\n",
      "validation loss 0.14181846716055715\n",
      "want 0.14181846716055718\n",
      "training loss 0.1588485836835828\n",
      "training accuracy 95.40833333333333\n",
      "epoch 494\n",
      "validation loss 0.1415956045769786\n",
      "want 0.1415956045769786\n",
      "training loss 0.15862767785764711\n",
      "training accuracy 95.40833333333333\n",
      "epoch 495\n",
      "validation loss 0.1413743399261057\n",
      "want 0.14137433992610574\n",
      "training loss 0.1584083749909959\n",
      "training accuracy 95.42500000000001\n",
      "epoch 496\n",
      "validation loss 0.1411546571386105\n",
      "want 0.1411546571386103\n",
      "training loss 0.158190659595286\n",
      "training accuracy 95.42500000000001\n",
      "epoch 497\n",
      "validation loss 0.1409365403317935\n",
      "want 0.14093654033179365\n",
      "training loss 0.15797451635276982\n",
      "training accuracy 95.43333333333334\n",
      "epoch 498\n",
      "validation loss 0.14071997380731666\n",
      "want 0.1407199738073167\n",
      "training loss 0.15775993011432293\n",
      "training accuracy 95.43333333333334\n",
      "epoch 499\n",
      "validation loss 0.14050494204895528\n",
      "want 0.14050494204895508\n",
      "training loss 0.15754688589748866\n",
      "training accuracy 95.44166666666666\n",
      "epoch 500\n",
      "validation loss 0.14029142972037859\n",
      "want 0.14029142972037845\n",
      "training loss 0.1573353688845578\n",
      "training accuracy 95.45\n",
      "epoch 501\n",
      "validation loss 0.14007942166295564\n",
      "want 0.14007942166295576\n",
      "training loss 0.1571253644206558\n",
      "training accuracy 95.45\n",
      "epoch 502\n",
      "validation loss 0.13986890289358428\n",
      "want 0.1398689028935844\n",
      "training loss 0.15691685801184635\n",
      "training accuracy 95.45833333333333\n",
      "epoch 503\n",
      "validation loss 0.13965985860254423\n",
      "want 0.1396598586025443\n",
      "training loss 0.15670983532327565\n",
      "training accuracy 95.45833333333333\n",
      "epoch 504\n",
      "validation loss 0.13945227415137637\n",
      "want 0.13945227415137645\n",
      "training loss 0.15650428217731527\n",
      "training accuracy 95.46666666666667\n",
      "epoch 505\n",
      "validation loss 0.1392461350707857\n",
      "want 0.13924613507078556\n",
      "training loss 0.15630018455173525\n",
      "training accuracy 95.475\n",
      "epoch 506\n",
      "validation loss 0.1390414270585671\n",
      "want 0.1390414270585671\n",
      "training loss 0.1560975285778955\n",
      "training accuracy 95.46666666666667\n",
      "epoch 507\n",
      "validation loss 0.13883813597755754\n",
      "want 0.13883813597755762\n",
      "training loss 0.15589630053895595\n",
      "training accuracy 95.46666666666667\n",
      "epoch 508\n",
      "validation loss 0.13863624785360987\n",
      "want 0.1386362478536098\n",
      "training loss 0.15569648686810988\n",
      "training accuracy 95.46666666666667\n",
      "epoch 509\n",
      "validation loss 0.13843574887359036\n",
      "want 0.13843574887359048\n",
      "training loss 0.15549807414682776\n",
      "training accuracy 95.46666666666667\n",
      "epoch 510\n",
      "validation loss 0.13823662538340234\n",
      "want 0.1382366253834022\n",
      "training loss 0.15530104910313214\n",
      "training accuracy 95.46666666666667\n",
      "epoch 511\n",
      "validation loss 0.1380388638860284\n",
      "want 0.1380388638860281\n",
      "training loss 0.15510539860988326\n",
      "training accuracy 95.46666666666667\n",
      "epoch 512\n",
      "validation loss 0.13784245103959974\n",
      "want 0.13784245103959974\n",
      "training loss 0.1549111096830852\n",
      "training accuracy 95.475\n",
      "epoch 513\n",
      "validation loss 0.1376473736554882\n",
      "want 0.13764737365548826\n",
      "training loss 0.15471816948021627\n",
      "training accuracy 95.475\n",
      "epoch 514\n",
      "validation loss 0.13745361869641753\n",
      "want 0.13745361869641742\n",
      "training loss 0.1545265652985717\n",
      "training accuracy 95.475\n",
      "epoch 515\n",
      "validation loss 0.1372611732745999\n",
      "want 0.13726117327460002\n",
      "training loss 0.1543362845736262\n",
      "training accuracy 95.475\n",
      "epoch 516\n",
      "validation loss 0.13707002464989596\n",
      "want 0.13707002464989604\n",
      "training loss 0.15414731487741942\n",
      "training accuracy 95.48333333333333\n",
      "epoch 517\n",
      "validation loss 0.13688016022799293\n",
      "want 0.136880160227993\n",
      "training loss 0.15395964391695344\n",
      "training accuracy 95.49166666666666\n",
      "epoch 518\n",
      "validation loss 0.13669156755860853\n",
      "want 0.13669156755860853\n",
      "training loss 0.15377325953261428\n",
      "training accuracy 95.49166666666666\n",
      "epoch 519\n",
      "validation loss 0.13650423433371436\n",
      "want 0.13650423433371447\n",
      "training loss 0.15358814969661044\n",
      "training accuracy 95.5\n",
      "epoch 520\n",
      "validation loss 0.13631814838578227\n",
      "want 0.13631814838578224\n",
      "training loss 0.15340430251142073\n",
      "training accuracy 95.49166666666666\n",
      "epoch 521\n",
      "validation loss 0.13613329768605015\n",
      "want 0.13613329768605018\n",
      "training loss 0.15322170620827577\n",
      "training accuracy 95.49166666666666\n",
      "epoch 522\n",
      "validation loss 0.13594967034281114\n",
      "want 0.1359496703428112\n",
      "training loss 0.15304034914564554\n",
      "training accuracy 95.5\n",
      "epoch 523\n",
      "validation loss 0.135767254599722\n",
      "want 0.13576725459972208\n",
      "training loss 0.15286021980774037\n",
      "training accuracy 95.50833333333333\n",
      "epoch 524\n",
      "validation loss 0.13558603883413314\n",
      "want 0.13558603883413312\n",
      "training loss 0.1526813068030508\n",
      "training accuracy 95.51666666666667\n",
      "epoch 525\n",
      "validation loss 0.1354060115554384\n",
      "want 0.13540601155543827\n",
      "training loss 0.15250359886287398\n",
      "training accuracy 95.51666666666667\n",
      "epoch 526\n",
      "validation loss 0.13522716140344584\n",
      "want 0.13522716140344582\n",
      "training loss 0.15232708483988555\n",
      "training accuracy 95.51666666666667\n",
      "epoch 527\n",
      "validation loss 0.13504947714676893\n",
      "want 0.13504947714676893\n",
      "training loss 0.15215175370670958\n",
      "training accuracy 95.51666666666667\n",
      "epoch 528\n",
      "validation loss 0.13487294768123637\n",
      "want 0.13487294768123634\n",
      "training loss 0.15197759455451088\n",
      "training accuracy 95.51666666666667\n",
      "epoch 529\n",
      "validation loss 0.13469756202832225\n",
      "want 0.1346975620283222\n",
      "training loss 0.15180459659161158\n",
      "training accuracy 95.51666666666667\n",
      "epoch 530\n",
      "validation loss 0.1345233093335967\n",
      "want 0.13452330933359655\n",
      "training loss 0.1516327491421088\n",
      "training accuracy 95.5\n",
      "epoch 531\n",
      "validation loss 0.13435017886519363\n",
      "want 0.13435017886519368\n",
      "training loss 0.15146204164452104\n",
      "training accuracy 95.525\n",
      "epoch 532\n",
      "validation loss 0.1341781600123004\n",
      "want 0.13417816001230043\n",
      "training loss 0.1512924636504467\n",
      "training accuracy 95.53333333333333\n",
      "epoch 533\n",
      "validation loss 0.13400724228366342\n",
      "want 0.1340072422836634\n",
      "training loss 0.15112400482323674\n",
      "training accuracy 95.53333333333333\n",
      "epoch 534\n",
      "validation loss 0.13383741530611454\n",
      "want 0.13383741530611445\n",
      "training loss 0.15095665493668442\n",
      "training accuracy 95.53333333333333\n",
      "epoch 535\n",
      "validation loss 0.13366866882311476\n",
      "want 0.13366866882311482\n",
      "training loss 0.15079040387373369\n",
      "training accuracy 95.55\n",
      "epoch 536\n",
      "validation loss 0.1335009926933176\n",
      "want 0.13350099269331753\n",
      "training loss 0.15062524162519889\n",
      "training accuracy 95.55833333333334\n",
      "epoch 537\n",
      "validation loss 0.13333437688914823\n",
      "want 0.1333343768891481\n",
      "training loss 0.1504611582884964\n",
      "training accuracy 95.55833333333334\n",
      "epoch 538\n",
      "validation loss 0.13316881149540252\n",
      "want 0.13316881149540258\n",
      "training loss 0.1502981440664037\n",
      "training accuracy 95.55833333333334\n",
      "epoch 539\n",
      "validation loss 0.13300428670786388\n",
      "want 0.13300428670786382\n",
      "training loss 0.15013618926582203\n",
      "training accuracy 95.56666666666666\n",
      "epoch 540\n",
      "validation loss 0.13284079283193495\n",
      "want 0.13284079283193487\n",
      "training loss 0.1499752842965548\n",
      "training accuracy 95.575\n",
      "epoch 541\n",
      "validation loss 0.1326783202812898\n",
      "want 0.13267832028128973\n",
      "training loss 0.1498154196701097\n",
      "training accuracy 95.58333333333333\n",
      "epoch 542\n",
      "validation loss 0.1325168595765411\n",
      "want 0.1325168595765412\n",
      "training loss 0.14965658599850362\n",
      "training accuracy 95.58333333333333\n",
      "epoch 543\n",
      "validation loss 0.13235640134392546\n",
      "want 0.1323564013439256\n",
      "training loss 0.14949877399308986\n",
      "training accuracy 95.58333333333333\n",
      "epoch 544\n",
      "validation loss 0.13219693631400417\n",
      "want 0.13219693631400423\n",
      "training loss 0.14934197446339548\n",
      "training accuracy 95.59166666666667\n",
      "epoch 545\n",
      "validation loss 0.13203845532038117\n",
      "want 0.13203845532038122\n",
      "training loss 0.14918617831597897\n",
      "training accuracy 95.6\n",
      "epoch 546\n",
      "validation loss 0.1318809492984376\n",
      "want 0.13188094929843766\n",
      "training loss 0.14903137655328771\n",
      "training accuracy 95.60833333333333\n",
      "epoch 547\n",
      "validation loss 0.13172440928408174\n",
      "want 0.13172440928408186\n",
      "training loss 0.14887756027254792\n",
      "training accuracy 95.60833333333333\n",
      "epoch 548\n",
      "validation loss 0.13156882641251563\n",
      "want 0.13156882641251566\n",
      "training loss 0.14872472066465517\n",
      "training accuracy 95.60833333333333\n",
      "epoch 549\n",
      "validation loss 0.13141419191701634\n",
      "want 0.13141419191701637\n",
      "training loss 0.1485728490130791\n",
      "training accuracy 95.60833333333333\n",
      "epoch 550\n",
      "validation loss 0.1312604971277345\n",
      "want 0.13126049712773416\n",
      "training loss 0.14842193669278847\n",
      "training accuracy 95.60833333333333\n",
      "epoch 551\n",
      "validation loss 0.13110773347050475\n",
      "want 0.13110773347050486\n",
      "training loss 0.14827197516918458\n",
      "training accuracy 95.60833333333333\n",
      "epoch 552\n",
      "validation loss 0.13095589246567782\n",
      "want 0.13095589246567796\n",
      "training loss 0.14812295599704287\n",
      "training accuracy 95.61666666666667\n",
      "epoch 553\n",
      "validation loss 0.1308049657269595\n",
      "want 0.13080496572695957\n",
      "training loss 0.14797487081948169\n",
      "training accuracy 95.61666666666667\n",
      "epoch 554\n",
      "validation loss 0.13065494496027025\n",
      "want 0.1306549449602703\n",
      "training loss 0.14782771136692693\n",
      "training accuracy 95.63333333333334\n",
      "epoch 555\n",
      "validation loss 0.1305058219626175\n",
      "want 0.1305058219626175\n",
      "training loss 0.14768146945609925\n",
      "training accuracy 95.63333333333334\n",
      "epoch 556\n",
      "validation loss 0.13035758862098215\n",
      "want 0.1303575886209821\n",
      "training loss 0.14753613698901605\n",
      "training accuracy 95.64166666666667\n",
      "epoch 557\n",
      "validation loss 0.13021023691121975\n",
      "want 0.13021023691121983\n",
      "training loss 0.1473917059519927\n",
      "training accuracy 95.63333333333334\n",
      "epoch 558\n",
      "validation loss 0.13006375889697636\n",
      "want 0.13006375889697624\n",
      "training loss 0.1472481684146681\n",
      "training accuracy 95.63333333333334\n",
      "epoch 559\n",
      "validation loss 0.12991814672861604\n",
      "want 0.12991814672861596\n",
      "training loss 0.14710551652904388\n",
      "training accuracy 95.63333333333334\n",
      "epoch 560\n",
      "validation loss 0.12977339264216545\n",
      "want 0.1297733926421655\n",
      "training loss 0.1469637425285222\n",
      "training accuracy 95.64166666666667\n",
      "epoch 561\n",
      "validation loss 0.1296294889582695\n",
      "want 0.12962948895826948\n",
      "training loss 0.14682283872696497\n",
      "training accuracy 95.64166666666667\n",
      "epoch 562\n",
      "validation loss 0.1294864280811607\n",
      "want 0.1294864280811608\n",
      "training loss 0.1466827975177614\n",
      "training accuracy 95.65\n",
      "epoch 563\n",
      "validation loss 0.12934420249764333\n",
      "want 0.12934420249764336\n",
      "training loss 0.14654361137291663\n",
      "training accuracy 95.65\n",
      "epoch 564\n",
      "validation loss 0.1292028047760884\n",
      "want 0.12920280477608845\n",
      "training loss 0.14640527284212926\n",
      "training accuracy 95.65\n",
      "epoch 565\n",
      "validation loss 0.12906222756544342\n",
      "want 0.12906222756544344\n",
      "training loss 0.1462677745519063\n",
      "training accuracy 95.65833333333333\n",
      "epoch 566\n",
      "validation loss 0.1289224635942539\n",
      "want 0.12892246359425405\n",
      "training loss 0.14613110920466854\n",
      "training accuracy 95.65833333333333\n",
      "epoch 567\n",
      "validation loss 0.12878350566969826\n",
      "want 0.12878350566969823\n",
      "training loss 0.14599526957788198\n",
      "training accuracy 95.65833333333333\n",
      "epoch 568\n",
      "validation loss 0.1286453466766333\n",
      "want 0.1286453466766331\n",
      "training loss 0.1458602485231855\n",
      "training accuracy 95.65833333333333\n",
      "epoch 569\n",
      "validation loss 0.128507979576654\n",
      "want 0.1285079795766541\n",
      "training loss 0.14572603896554237\n",
      "training accuracy 95.65833333333333\n",
      "epoch 570\n",
      "validation loss 0.12837139740716605\n",
      "want 0.12837139740716597\n",
      "training loss 0.14559263390239538\n",
      "training accuracy 95.675\n",
      "epoch 571\n",
      "validation loss 0.12823559328046616\n",
      "want 0.1282355932804659\n",
      "training loss 0.14546002640282996\n",
      "training accuracy 95.68333333333334\n",
      "epoch 572\n",
      "validation loss 0.12810056038283854\n",
      "want 0.12810056038283849\n",
      "training loss 0.1453282096067588\n",
      "training accuracy 95.675\n",
      "epoch 573\n",
      "validation loss 0.12796629197366216\n",
      "want 0.12796629197366213\n",
      "training loss 0.14519717672410076\n",
      "training accuracy 95.675\n",
      "epoch 574\n",
      "validation loss 0.12783278138452717\n",
      "want 0.12783278138452722\n",
      "training loss 0.14506692103398255\n",
      "training accuracy 95.69166666666666\n",
      "epoch 575\n",
      "validation loss 0.12770002201836542\n",
      "want 0.12770002201836542\n",
      "training loss 0.14493743588394264\n",
      "training accuracy 95.69166666666666\n",
      "epoch 576\n",
      "validation loss 0.1275680073485904\n",
      "want 0.12756800734859036\n",
      "training loss 0.14480871468915504\n",
      "training accuracy 95.7\n",
      "epoch 577\n",
      "validation loss 0.1274367309182492\n",
      "want 0.12743673091824917\n",
      "training loss 0.1446807509316452\n",
      "training accuracy 95.70833333333333\n",
      "epoch 578\n",
      "validation loss 0.12730618633918517\n",
      "want 0.12730618633918522\n",
      "training loss 0.14455353815953204\n",
      "training accuracy 95.7\n",
      "epoch 579\n",
      "validation loss 0.1271763672912113\n",
      "want 0.12717636729121126\n",
      "training loss 0.14442706998627336\n",
      "training accuracy 95.7\n",
      "epoch 580\n",
      "validation loss 0.12704726752129353\n",
      "want 0.12704726752129353\n",
      "training loss 0.14430134008991832\n",
      "training accuracy 95.7\n",
      "epoch 581\n",
      "validation loss 0.1269188808427459\n",
      "want 0.1269188808427463\n",
      "training loss 0.14417634221237235\n",
      "training accuracy 95.70833333333333\n",
      "epoch 582\n",
      "validation loss 0.12679120113443654\n",
      "want 0.12679120113443648\n",
      "training loss 0.14405207015866794\n",
      "training accuracy 95.71666666666667\n",
      "epoch 583\n",
      "validation loss 0.12666422233999916\n",
      "want 0.1266642223399989\n",
      "training loss 0.14392851779625004\n",
      "training accuracy 95.71666666666667\n",
      "epoch 584\n",
      "validation loss 0.12653793846706157\n",
      "want 0.12653793846706143\n",
      "training loss 0.14380567905426128\n",
      "training accuracy 95.72500000000001\n",
      "epoch 585\n",
      "validation loss 0.12641234358647999\n",
      "want 0.12641234358647996\n",
      "training loss 0.14368354792284665\n",
      "training accuracy 95.72500000000001\n",
      "epoch 586\n",
      "validation loss 0.12628743183158356\n",
      "want 0.12628743183158347\n",
      "training loss 0.1435621184524558\n",
      "training accuracy 95.72500000000001\n",
      "epoch 587\n",
      "validation loss 0.12616319739742876\n",
      "want 0.12616319739742868\n",
      "training loss 0.14344138475316542\n",
      "training accuracy 95.72500000000001\n",
      "epoch 588\n",
      "validation loss 0.12603963454006425\n",
      "want 0.12603963454006414\n",
      "training loss 0.14332134099399768\n",
      "training accuracy 95.72500000000001\n",
      "epoch 589\n",
      "validation loss 0.12591673757580402\n",
      "want 0.12591673757580407\n",
      "training loss 0.14320198140225732\n",
      "training accuracy 95.73333333333333\n",
      "epoch 590\n",
      "validation loss 0.1257945008805112\n",
      "want 0.12579450088051117\n",
      "training loss 0.14308330026287222\n",
      "training accuracy 95.73333333333333\n",
      "epoch 591\n",
      "validation loss 0.12567291888888876\n",
      "want 0.12567291888888896\n",
      "training loss 0.1429652919177418\n",
      "training accuracy 95.73333333333333\n",
      "epoch 592\n",
      "validation loss 0.12555198609378293\n",
      "want 0.12555198609378293\n",
      "training loss 0.14284795076509155\n",
      "training accuracy 95.73333333333333\n",
      "epoch 593\n",
      "validation loss 0.12543169704549084\n",
      "want 0.12543169704549098\n",
      "training loss 0.14273127125884386\n",
      "training accuracy 95.73333333333333\n",
      "epoch 594\n",
      "validation loss 0.1253120463510822\n",
      "want 0.12531204635108212\n",
      "training loss 0.14261524790798888\n",
      "training accuracy 95.74166666666667\n",
      "epoch 595\n",
      "validation loss 0.1251930286737247\n",
      "want 0.12519302867372462\n",
      "training loss 0.1424998752759608\n",
      "training accuracy 95.74166666666667\n",
      "epoch 596\n",
      "validation loss 0.12507463873202215\n",
      "want 0.1250746387320222\n",
      "training loss 0.14238514798003124\n",
      "training accuracy 95.74166666666667\n",
      "epoch 597\n",
      "validation loss 0.12495687129935892\n",
      "want 0.1249568712993591\n",
      "training loss 0.14227106069070206\n",
      "training accuracy 95.74166666666667\n",
      "epoch 598\n",
      "validation loss 0.12483972120325332\n",
      "want 0.12483972120325322\n",
      "training loss 0.14215760813111125\n",
      "training accuracy 95.74166666666667\n",
      "epoch 599\n",
      "validation loss 0.12472318332471768\n",
      "want 0.12472318332471767\n",
      "training loss 0.14204478507643825\n",
      "training accuracy 95.74166666666667\n",
      "epoch 600\n",
      "validation loss 0.12460725259763074\n",
      "want 0.1246072525976307\n",
      "training loss 0.14193258635332645\n",
      "training accuracy 95.74166666666667\n",
      "epoch 601\n",
      "validation loss 0.12449192400811328\n",
      "want 0.12449192400811315\n",
      "training loss 0.1418210068393029\n",
      "training accuracy 95.74166666666667\n",
      "epoch 602\n",
      "validation loss 0.12437719259391455\n",
      "want 0.12437719259391454\n",
      "training loss 0.14171004146221797\n",
      "training accuracy 95.74166666666667\n",
      "epoch 603\n",
      "validation loss 0.12426305344380628\n",
      "want 0.12426305344380628\n",
      "training loss 0.1415996851996748\n",
      "training accuracy 95.75\n",
      "epoch 604\n",
      "validation loss 0.1241495016969834\n",
      "want 0.12414950169698323\n",
      "training loss 0.14148993307847832\n",
      "training accuracy 95.75\n",
      "epoch 605\n",
      "validation loss 0.12403653254247246\n",
      "want 0.12403653254247254\n",
      "training loss 0.14138078017408987\n",
      "training accuracy 95.75\n",
      "epoch 606\n",
      "validation loss 0.12392414121854996\n",
      "want 0.12392414121855012\n",
      "training loss 0.141272221610082\n",
      "training accuracy 95.75\n",
      "epoch 607\n",
      "validation loss 0.12381232301216455\n",
      "want 0.12381232301216454\n",
      "training loss 0.14116425255760967\n",
      "training accuracy 95.73333333333333\n",
      "epoch 608\n",
      "validation loss 0.12370107325836818\n",
      "want 0.12370107325836831\n",
      "training loss 0.14105686823487346\n",
      "training accuracy 95.73333333333333\n",
      "epoch 609\n",
      "validation loss 0.12359038733975629\n",
      "want 0.12359038733975639\n",
      "training loss 0.1409500639066061\n",
      "training accuracy 95.73333333333333\n",
      "epoch 610\n",
      "validation loss 0.12348026068591154\n",
      "want 0.12348026068591171\n",
      "training loss 0.1408438348835531\n",
      "training accuracy 95.74166666666667\n",
      "epoch 611\n",
      "validation loss 0.12337068877285796\n",
      "want 0.12337068877285796\n",
      "training loss 0.14073817652196588\n",
      "training accuracy 95.74166666666667\n",
      "epoch 612\n",
      "validation loss 0.12326166712251892\n",
      "want 0.12326166712251901\n",
      "training loss 0.14063308422309628\n",
      "training accuracy 95.74166666666667\n",
      "epoch 613\n",
      "validation loss 0.1231531913021858\n",
      "want 0.12315319130218572\n",
      "training loss 0.14052855343270265\n",
      "training accuracy 95.74166666666667\n",
      "epoch 614\n",
      "validation loss 0.12304525692398892\n",
      "want 0.12304525692398886\n",
      "training loss 0.14042457964055607\n",
      "training accuracy 95.74166666666667\n",
      "epoch 615\n",
      "validation loss 0.12293785964437937\n",
      "want 0.12293785964437923\n",
      "training loss 0.1403211583799593\n",
      "training accuracy 95.74166666666667\n",
      "epoch 616\n",
      "validation loss 0.12283099516361388\n",
      "want 0.12283099516361412\n",
      "training loss 0.14021828522726423\n",
      "training accuracy 95.75\n",
      "epoch 617\n",
      "validation loss 0.12272465922525028\n",
      "want 0.12272465922525029\n",
      "training loss 0.14011595580140232\n",
      "training accuracy 95.75\n",
      "epoch 618\n",
      "validation loss 0.12261884761564353\n",
      "want 0.12261884761564354\n",
      "training loss 0.14001416576341275\n",
      "training accuracy 95.75\n",
      "epoch 619\n",
      "validation loss 0.12251355616345422\n",
      "want 0.12251355616345433\n",
      "training loss 0.13991291081598733\n",
      "training accuracy 95.74166666666667\n",
      "epoch 620\n",
      "validation loss 0.12240878073915978\n",
      "want 0.12240878073915983\n",
      "training loss 0.13981218670300763\n",
      "training accuracy 95.75\n",
      "epoch 621\n",
      "validation loss 0.12230451725457207\n",
      "want 0.12230451725457211\n",
      "training loss 0.1397119892090972\n",
      "training accuracy 95.75833333333334\n",
      "epoch 622\n",
      "validation loss 0.1222007616623623\n",
      "want 0.12220076166236246\n",
      "training loss 0.13961231415917688\n",
      "training accuracy 95.76666666666667\n",
      "epoch 623\n",
      "validation loss 0.1220975099555916\n",
      "want 0.12209750995559149\n",
      "training loss 0.13951315741802392\n",
      "training accuracy 95.76666666666667\n",
      "epoch 624\n",
      "validation loss 0.12199475816724549\n",
      "want 0.12199475816724543\n",
      "training loss 0.13941451488983977\n",
      "training accuracy 95.775\n",
      "epoch 625\n",
      "validation loss 0.12189250236977801\n",
      "want 0.12189250236977804\n",
      "training loss 0.13931638251781533\n",
      "training accuracy 95.775\n",
      "epoch 626\n",
      "validation loss 0.1217907386746586\n",
      "want 0.12179073867465855\n",
      "training loss 0.1392187562837139\n",
      "training accuracy 95.775\n",
      "epoch 627\n",
      "validation loss 0.12168946323192491\n",
      "want 0.12168946323192488\n",
      "training loss 0.1391216322074462\n",
      "training accuracy 95.78333333333333\n",
      "epoch 628\n",
      "validation loss 0.12158867222974294\n",
      "want 0.12158867222974291\n",
      "training loss 0.13902500634666026\n",
      "training accuracy 95.78333333333333\n",
      "epoch 629\n",
      "validation loss 0.12148836189397096\n",
      "want 0.12148836189397101\n",
      "training loss 0.1389288747963318\n",
      "training accuracy 95.78333333333333\n",
      "epoch 630\n",
      "validation loss 0.12138852848773016\n",
      "want 0.12138852848773016\n",
      "training loss 0.1388332336883561\n",
      "training accuracy 95.78333333333333\n",
      "epoch 631\n",
      "validation loss 0.12128916831097915\n",
      "want 0.12128916831097926\n",
      "training loss 0.138738079191152\n",
      "training accuracy 95.78333333333333\n",
      "epoch 632\n",
      "validation loss 0.1211902777000961\n",
      "want 0.12119027770009606\n",
      "training loss 0.13864340750927223\n",
      "training accuracy 95.78333333333333\n",
      "epoch 633\n",
      "validation loss 0.12109185302746302\n",
      "want 0.12109185302746316\n",
      "training loss 0.13854921488300173\n",
      "training accuracy 95.78333333333333\n",
      "epoch 634\n",
      "validation loss 0.12099389070105902\n",
      "want 0.12099389070105912\n",
      "training loss 0.13845549758798373\n",
      "training accuracy 95.78333333333333\n",
      "epoch 635\n",
      "validation loss 0.12089638716405487\n",
      "want 0.12089638716405482\n",
      "training loss 0.13836225193483204\n",
      "training accuracy 95.78333333333333\n",
      "epoch 636\n",
      "validation loss 0.12079933889441485\n",
      "want 0.12079933889441484\n",
      "training loss 0.13826947426876066\n",
      "training accuracy 95.78333333333333\n",
      "epoch 637\n",
      "validation loss 0.12070274240450383\n",
      "want 0.12070274240450378\n",
      "training loss 0.1381771609692058\n",
      "training accuracy 95.78333333333333\n",
      "epoch 638\n",
      "validation loss 0.12060659424069754\n",
      "want 0.12060659424069735\n",
      "training loss 0.13808530844946565\n",
      "training accuracy 95.78333333333333\n",
      "epoch 639\n",
      "validation loss 0.12051089098299855\n",
      "want 0.12051089098299865\n",
      "training loss 0.13799391315633053\n",
      "training accuracy 95.8\n",
      "epoch 640\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-133-c4d6395e4d4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_training\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_training\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-115-e99a2f93efd6>\u001b[0m in \u001b[0;36mmlp\u001b[0;34m(xs, ys, learning_rate, momentum, devxs, devys)\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mforward_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainxs_shuffled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainxs_shuffled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainys_shuffled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_res\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mups\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainxs_shuffled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-c7419f1677dc>\u001b[0m in \u001b[0;36mupdate_grads\u001b[0;34m(grads, ups, batch_size)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dW1\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mups\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dW1\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"db1\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mups\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"db1\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epoch, loss_training, accuracy_training, accuracy_validation = mlp(trainxs, trainys, learning_rate, momentum, devxs, devys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Loss plot for training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x118042f98>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.axis([0, epoch, min(loss_training) - 0.2, max(loss_training) + 0.005])\n",
    "plt.plot(loss_training, label='training set loss')\n",
    "plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Accuracy plot for training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x11860be48>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.axis([0, epoch, min(accuracy_training), max(accuracy_training) + 1])\n",
    "plt.plot(accuracy_training, label='training set accuracy')\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 Accuracy for best epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = accuracy_validation.index(max(accuracy_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best epoch = 126\n",
      "Accuracy on training set = 94.625\n",
      "Accuracy on validation set = 95.3\n"
     ]
    }
   ],
   "source": [
    "print(\"Best epoch =\", ind)\n",
    "print(\"Accuracy on training set =\", accuracy_training[ind])\n",
    "print(\"Accuracy on validation set =\", accuracy_validation[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
